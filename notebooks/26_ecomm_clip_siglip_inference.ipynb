{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38e5bbd-753f-4e9f-b591-f2196378fe01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pathlib\n",
    "\n",
    "import torch\n",
    "import clip\n",
    "import tqdm\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "\n",
    "def batch_images_generator(path_to_folder, bs, preprocessor, resume_from_batch=None, list_of_images=None):\n",
    "    if list_of_images:\n",
    "        images = list_of_images\n",
    "    else:\n",
    "        images = sorted(glob.glob('*.jpg', root_dir=path_to_folder))\n",
    "        images = [image for image in images if os.path.getsize(path_to_folder + image) > 0]\n",
    "    print(len(images))\n",
    "    start = 0 * bs if not resume_from_batch else resume_from_batch * bs\n",
    "    print(f'start from {start}')\n",
    "    for i in tqdm.tqdm(range(start, len(images), bs)):\n",
    "        batch_images = images[i:i + bs]\n",
    "        torch_batch = []\n",
    "        valid_images = []\n",
    "        for image in batch_images:\n",
    "            try:\n",
    "                torch_batch.append(\n",
    "                    preprocessor(images=Image.open(path_to_folder + image), return_tensors=\"pt\")['pixel_values']\n",
    "                )\n",
    "                valid_images.append(image)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(image)\n",
    "\n",
    "        torch_batch = torch.cat(torch_batch).to(device)\n",
    "\n",
    "        yield i, torch_batch, valid_images\n",
    "\n",
    "\n",
    "def clip_embeddings(model, batch):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embeddings = model.encode_image(batch)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def hf_embeddings(model, batch):\n",
    "    with torch.no_grad():\n",
    "        image_features = model.get_image_features(batch, normalize=True)\n",
    "    return image_features\n",
    "\n",
    "\n",
    "def get_missing_images(path_to_parquet, path_to_valid_images):\n",
    "    with open(path_to_valid_images, 'r') as f:\n",
    "        valid_images = list(map(lambda x: x.strip(), f.readlines()))\n",
    "\n",
    "    embedded_images = pd.read_parquet(path_to_parquet, columns=['filename']).filename.values\n",
    "\n",
    "    return list(set(valid_images) - set(embedded_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df404add-684b-48a4-b66b-d9d180e63b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoProcessor\n",
    "\n",
    "model_name = 'Marqo/marqo-fashionSigLIP' # Marqo/marqo-ecommerce-embeddings-L\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True).to(device)\n",
    "processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "batch_size = 512\n",
    "for t in ['train', 'test']:\n",
    "    output_dir = 'parquets'\n",
    "    #os.mkdir(output_dir, exist)\n",
    "    #os.mkdir(output_dir, exist_ok=True)\n",
    "    images_dir = f'avito/images/{t}/images/'\n",
    "    images_list = get_missing_images('/avito/images/embeddings/final_embeddings_fashion_clip_train.parquet',\n",
    "                                     './valid_train_images.txt')\n",
    "\n",
    "    for i, batch, fname in batch_images_generator(images_dir,\n",
    "                                                  bs=batch_size,\n",
    "                                                  preprocessor=processor,\n",
    "                                                  resume_from_batch=0,\n",
    "                                                  list_of_images=images_list):\n",
    "\n",
    "        embedded = hf_embeddings(model, batch).tolist()\n",
    "\n",
    "        df = pd.DataFrame({\n",
    "            \"filename\": fname,\n",
    "            \"embedding\": embedded\n",
    "        })\n",
    "\n",
    "        df.to_parquet(\n",
    "            os.path.join(output_dir, f\"batch_{i // batch_size}.parquet\"),\n",
    "            index=False\n",
    "        )\n",
    "\n",
    "    duckdb.sql(rf\"\"\"\n",
    "        COPY (\n",
    "            SELECT * FROM '{output_dir}/*.parquet'\n",
    "        )\n",
    "        TO '{output_dir}/final_embeddings.parquet' (FORMAT PARQUET)\n",
    "    \"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
