{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec05048e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "import scipy.sparse as sp\n",
    "import json\n",
    "import ast\n",
    "import pickle\n",
    "import gc\n",
    "import re\n",
    "\n",
    "from rank_bm25 import BM25Okapi\n",
    "from rouge import Rouge\n",
    "from rapidfuzz import fuzz\n",
    "import jellyfish\n",
    "import textdistance\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d975bb28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.shape=(1000, 102)\n"
     ]
    }
   ],
   "source": [
    "df = pl.read_parquet('../data/preprocessed/all_products_preprocessed.parquet')\n",
    "\n",
    "print(f'{df.shape=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d4bc4d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1420"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecfa04fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# коссим по тфидф векторам\n",
    "\n",
    "def add_cosine_similarity_from_files(\n",
    "    train_df: pl.DataFrame,\n",
    "    train_tpl_1: str,\n",
    "    train_tpl_2: str,\n",
    "    cols: list[str]\n",
    ") -> tuple[pl.DataFrame, pl.DataFrame]:\n",
    "    for col in cols:\n",
    "        print(f'{col=}')\n",
    "        \n",
    "        print('loading tr1')\n",
    "        tr1 = sp.load_npz(train_tpl_1.format(col=col))\n",
    "        \n",
    "        print('loading tr2')\n",
    "        tr2 = sp.load_npz(train_tpl_2.format(col=col))\n",
    "        \n",
    "\n",
    "        train_sims = []\n",
    "        for i in tqdm(range(tr1.shape[0]), desc=f'Train rows for {col}', leave=False):\n",
    "            if tr1.getrow(i).nnz or tr2.getrow(i).nnz:\n",
    "                sim = float(cosine_similarity(tr1.getrow(i), tr2.getrow(i))[0, 0])\n",
    "            else:\n",
    "                sim = 0.0\n",
    "            train_sims.append(sim)\n",
    "\n",
    "        sim_col = f\"{col}_cosine_sim\"\n",
    "        train_df = train_df.with_columns(pl.Series(sim_col, train_sims))\n",
    "\n",
    "        del train_sims\n",
    "        gc.collect()\n",
    "\n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f165767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "col='concat_keyval'\n",
      "loading tr1\n",
      "loading tr2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "col='name'\n",
      "loading tr1\n",
      "loading tr2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "col='description'\n",
      "loading tr1\n",
      "loading tr2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = add_cosine_similarity_from_files(\n",
    "    df,\n",
    "    '../data/preprocessed/{col}_1_tfidf_matrices.npz',\n",
    "    '../data/preprocessed/{col}_2_tfidf_matrices.npz',\n",
    "    ['concat_keyval', 'name', 'description']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94fe7804",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing token_sort_ratio for level 4: 100%|██████████| 1000/1000 [00:00<00:00, 135461.81it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# мэтч по категориям 1-4 уровня + частичный мэтч по 4 уровню\n",
    "\n",
    "for i in range(1, 5):\n",
    "    df = df.with_columns(\n",
    "        pl.col(f'category_level_{i}_1').cast(pl.String).str.to_lowercase().eq(\n",
    "            pl.col(f'category_level_{i}_2').cast(pl.String).str.to_lowercase()\n",
    "        ).alias(f'category_level_{i}_match')\n",
    "    )\n",
    "    \n",
    "    if i == 4:\n",
    "        def calc_token_sort_ratio(row):\n",
    "            val1 = str(row[f'category_level_4_1'])\n",
    "            val2 = str(row[f'category_level_4_2'])\n",
    "            return fuzz.token_sort_ratio(val1, val2) / 100\n",
    "        \n",
    "        total = len(df)\n",
    "        pbar = tqdm(total=total, desc=f'Processing token_sort_ratio for level {i}')\n",
    "        \n",
    "        def token_sort_with_progress(row):\n",
    "            pbar.update(1)\n",
    "            return calc_token_sort_ratio(row)\n",
    "        \n",
    "        df = df.with_columns(\n",
    "            pl.struct([f'category_level_4_1', f'category_level_4_2'])\n",
    "            .map_elements(token_sort_with_progress, return_dtype=pl.Float64)\n",
    "            .alias(f'category_level_4_token_sort_ratio_match')\n",
    "        )\n",
    "        pbar.close()\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a98f5478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# всего совпадений по категориям\n",
    "\n",
    "df = df.with_columns(\n",
    "    (pl.col('category_level_1_match').fill_null(False).cast(pl.Int64) + \n",
    "        pl.col('category_level_2_match').fill_null(False).cast(pl.Int64) + \n",
    "        pl.col('category_level_3_match').fill_null(False).cast(pl.Int64) + \n",
    "        pl.col('category_level_4_match').fill_null(False).cast(pl.Int64))\n",
    "    .alias('category_total_matches')\n",
    ")\n",
    "    \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5bfc0d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing price ratios: 100%|██████████| 1000/1000 [00:00<00:00, 313171.36it/s]\n",
      "Processing n_images ratios: 100%|██████████| 1000/1000 [00:00<00:00, 357784.18it/s]\n",
      "Processing name_tokens_len ratios: 100%|██████████| 1000/1000 [00:00<00:00, 367470.12it/s]\n",
      "Processing description_tokens_len ratios: 100%|██████████| 1000/1000 [00:00<00:00, 378786.60it/s]\n",
      "Processing name_en_tokens_len ratios: 100%|██████████| 1000/1000 [00:00<00:00, 348161.70it/s]\n",
      "Processing description_en_tokens_len ratios: 100%|██████████| 1000/1000 [00:00<00:00, 373857.21it/s]\n",
      "Processing name_mix_tokens_len ratios: 100%|██████████| 1000/1000 [00:00<00:00, 370881.95it/s]\n",
      "Processing description_mix_tokens_len ratios: 100%|██████████| 1000/1000 [00:00<00:00, 359439.88it/s]\n",
      "Processing attr_keys_len ratios: 100%|██████████| 1000/1000 [00:00<00:00, 388253.63it/s]\n",
      "Processing attr_vals_len ratios: 100%|██████████| 1000/1000 [00:00<00:00, 359224.39it/s]\n",
      "Processing name_tokens_w_digits ratios: 100%|██████████| 1000/1000 [00:00<00:00, 274030.05it/s]\n",
      "Processing description_tokens_w_digits ratios: 100%|██████████| 1000/1000 [00:00<00:00, 200224.56it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# отношения длин для всего подряд (левые и правые)\n",
    "\n",
    "num_cols = [\n",
    "    'price', 'n_images', \n",
    "    'name_tokens_len', 'description_tokens_len', \n",
    "    'name_en_tokens_len', 'description_en_tokens_len',\n",
    "    'name_mix_tokens_len', 'description_mix_tokens_len',\n",
    "    'attr_keys_len', 'attr_vals_len',\n",
    "]\n",
    "\n",
    "str_cols = ['name_tokens_w_digits', 'description_tokens_w_digits']\n",
    "\n",
    "for col in num_cols:\n",
    "    total = len(df)\n",
    "    pbar = tqdm(total=total, desc=f\"Processing {col} ratios\")\n",
    "\n",
    "    def ratio_left(row):\n",
    "        pbar.update(1)\n",
    "        val2 = row[f'{col}_2']\n",
    "        val1 = row[f'{col}_1']\n",
    "        \n",
    "        if val2 is None or val2 == 0 or val1 is None:\n",
    "            return None\n",
    "        \n",
    "        return float(val1) / float(val2)\n",
    "\n",
    "    def ratio_right(row):\n",
    "        val2 = row[f'{col}_2']\n",
    "        val1 = row[f'{col}_1']\n",
    "        \n",
    "        if val1 is None or val1 == 0 or val2 is None:\n",
    "            return None\n",
    "            \n",
    "        return float(val2) / float(val1)\n",
    "\n",
    "    df = df.with_columns(\n",
    "        pl.struct([f'{col}_1', f'{col}_2']).map_elements(\n",
    "            ratio_left, return_dtype=pl.Float64\n",
    "        ).alias(f'{col}_ratio_left'),\n",
    "        \n",
    "        pl.struct([f'{col}_1', f'{col}_2']).map_elements(\n",
    "            ratio_right, return_dtype=pl.Float64\n",
    "        ).alias(f'{col}_ratio_right')\n",
    "    )\n",
    "    pbar.close()\n",
    "\n",
    "for col in str_cols:\n",
    "    total = len(df)\n",
    "    pbar = tqdm(total=total, desc=f\"Processing {col} ratios\")\n",
    "\n",
    "    def str_ratio_left(row):\n",
    "        pbar.update(1)\n",
    "        val1 = row[f'{col}_1']\n",
    "        val2 = row[f'{col}_2']\n",
    "        \n",
    "        if val1 is None:\n",
    "            len1 = 0\n",
    "        else:\n",
    "            len1 = len(str(val1).split())\n",
    "            \n",
    "        if val2 is None:\n",
    "            len2 = 0\n",
    "        else:\n",
    "            len2 = len(str(val2).split())\n",
    "            \n",
    "        if len2 == 0:\n",
    "            return 0.0\n",
    "            \n",
    "        return float(len1) / float(len2)\n",
    "\n",
    "    def str_ratio_right(row):\n",
    "        val1 = row[f'{col}_1']\n",
    "        val2 = row[f'{col}_2']\n",
    "        \n",
    "        if val1 is None:\n",
    "            len1 = 0\n",
    "        else:\n",
    "            len1 = len(str(val1).split())\n",
    "            \n",
    "        if val2 is None:\n",
    "            len2 = 0\n",
    "        else:\n",
    "            len2 = len(str(val2).split())\n",
    "            \n",
    "        if len1 == 0:\n",
    "            return 0.0\n",
    "            \n",
    "        return float(len2) / float(len1)\n",
    "\n",
    "    df = df.with_columns(\n",
    "        pl.struct([f'{col}_1', f'{col}_2']).map_elements(\n",
    "            str_ratio_left, return_dtype=pl.Float64\n",
    "        ).alias(f'{col}_ratio_left'),\n",
    "        \n",
    "        pl.struct([f'{col}_1', f'{col}_2']).map_elements(\n",
    "            str_ratio_right, return_dtype=pl.Float64\n",
    "        ).alias(f'{col}_ratio_right')\n",
    "    )\n",
    "    pbar.close()\n",
    "    \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7afa4839",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing abs_diff for price: 100%|██████████| 1000/1000 [00:00<00:00, 279638.91it/s]\n",
      "Processing abs_diff for n_images: 100%|██████████| 1000/1000 [00:00<00:00, 474039.78it/s]\n",
      "Processing abs_diff for attr_keys_len: 100%|██████████| 1000/1000 [00:00<00:00, 592750.71it/s]\n",
      "Processing abs_diff for attr_vals_len: 100%|██████████| 1000/1000 [00:00<00:00, 594009.91it/s]\n",
      "Processing sq_diff for price: 100%|██████████| 1000/1000 [00:00<00:00, 385010.46it/s]\n",
      "Processing sq_diff for n_images: 100%|██████████| 1000/1000 [00:00<00:00, 513693.08it/s]\n",
      "Processing sq_diff for attr_keys_len: 100%|██████████| 1000/1000 [00:00<00:00, 512250.12it/s]\n",
      "Processing sq_diff for attr_vals_len: 100%|██████████| 1000/1000 [00:00<00:00, 518583.58it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# разницы длин\n",
    "\n",
    "def abs_len_diff_features(df, cols):\n",
    "    for col in cols:\n",
    "        total = len(df)\n",
    "        pbar = tqdm(total=total, desc=f\"Processing abs_diff for {col}\")\n",
    "\n",
    "        def abs_len_diff(row):\n",
    "            pbar.update(1)\n",
    "            val1 = row[f'{col}_1']\n",
    "            val2 = row[f'{col}_2']\n",
    "            if val1 is None or val2 is None:\n",
    "                return None\n",
    "            v1 = int(val1)\n",
    "            v2 = int(val2)\n",
    "            return abs(v1 - v2)\n",
    "\n",
    "        df = df.with_columns(\n",
    "            pl.struct([f'{col}_1', f'{col}_2'])\n",
    "            .map_elements(abs_len_diff, return_dtype=pl.Int64)\n",
    "            .alias(f'{col}_abs_diff')\n",
    "        )\n",
    "        pbar.close()\n",
    "        \n",
    "    return df\n",
    "\n",
    "def abs_len_sqdiff_features(df, cols):\n",
    "    for col in cols:\n",
    "        total = len(df)\n",
    "        pbar = tqdm(total=total, desc=f\"Processing sq_diff for {col}\")\n",
    "\n",
    "        def sq_len_diff(row):\n",
    "            pbar.update(1)\n",
    "            val1 = row[f'{col}_1']\n",
    "            val2 = row[f'{col}_2']\n",
    "            if val1 is None or val2 is None:\n",
    "                return None\n",
    "            v1 = int(val1)\n",
    "            v2 = int(val2)\n",
    "            \n",
    "            diff_sq = (v1 - v2) ** 2\n",
    "            \n",
    "            max_uint64 = 2 ** 64 - 1\n",
    "            if diff_sq >= max_uint64:\n",
    "                return max_uint64 - 1\n",
    "            \n",
    "            return diff_sq\n",
    "\n",
    "        df = df.with_columns(\n",
    "            pl.struct([f'{col}_1', f'{col}_2'])\n",
    "            .map_elements(sq_len_diff, return_dtype=pl.UInt64)\n",
    "            .alias(f'{col}_sq_diff')\n",
    "        )\n",
    "        pbar.close()\n",
    "        \n",
    "    return df\n",
    "\n",
    "diff_cols = ['price', 'n_images', 'attr_keys_len', 'attr_vals_len']\n",
    "\n",
    "df = abs_len_diff_features(df, diff_cols)\n",
    "df = abs_len_sqdiff_features(df, diff_cols)\n",
    "    \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "825fd1b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# полнота некоторых столбцов с пропусками\n",
    "\n",
    "def fillness(df: pl.DataFrame, col_name: str) -> pl.DataFrame:\n",
    "    condition_both = (pl.col(f'{col_name}_1').is_not_null() & \n",
    "                      pl.col(f'{col_name}_2').is_not_null())\n",
    "    condition_none = (pl.col(f'{col_name}_1').is_null() & \n",
    "                      pl.col(f'{col_name}_2').is_null())\n",
    "    \n",
    "    df = df.with_columns(\n",
    "        pl.when(condition_both).then(pl.lit('both'))\n",
    "        .when(condition_none).then(pl.lit('none'))\n",
    "        .otherwise(pl.lit('only one'))\n",
    "        .alias(f'{col_name}_fillness')\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = fillness(df, 'category_level_3')\n",
    "df = fillness(df, 'category_level_4')\n",
    "df = fillness(df, 'n_images')\n",
    "    \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd10b2a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# фичи на антисловах\n",
    "\n",
    "with open('../data/preprocessed/filtered_anti_words.pkl', 'rb') as file:\n",
    "    filtered_anti_words = pickle.load(file)\n",
    "\n",
    "N_TOP_ANTIWORD = 100\n",
    "top_anti_words = set([w[0] for w in filtered_anti_words.most_common(N_TOP_ANTIWORD)])\n",
    "\n",
    "def calc_anti_words_values(row: dict) -> float:\n",
    "    name1, name2 = row['name_1'], row['name_2']\n",
    "    words1 = set(re.findall(r'([a-z]+)', name1.lower()))\n",
    "    words2 = set(re.findall(r'([a-z]+)', name2.lower()))\n",
    "    \n",
    "    if not (words1 | words2):\n",
    "        return 0.0\n",
    "        \n",
    "    xor_words = words1.symmetric_difference(words2)\n",
    "    intersection = xor_words & top_anti_words\n",
    "    return len(intersection) / max(len(words1), len(words2))\n",
    "\n",
    "df = df.with_columns(\n",
    "    pl.struct(['name_1', 'name_2'])\n",
    "    .map_elements(calc_anti_words_values, return_dtype=pl.Float64)\n",
    "    .alias('anti_words_values')\n",
    ")\n",
    "    \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3aa9edaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating avg_fully_eq_attributes: 100%|██████████| 1000/1000 [00:00<00:00, 11718.00it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "308"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# совпадения для словаря атрибутов\n",
    "\n",
    "def avg_fully_eq_attributes(d1, d2):\n",
    "    if d1 is None or d2 is None:\n",
    "        return None\n",
    "    try:\n",
    "        d1 = ast.literal_eval(d1)\n",
    "        d2 = ast.literal_eval(d2)\n",
    "    except Exception:\n",
    "        print('bad!!!')\n",
    "        return None\n",
    "    keys = set(d1) & set(d2)\n",
    "    metrics = []\n",
    "    for key in keys:\n",
    "        metrics.append(set(d1[key]) == set(d2[key]))\n",
    "    if len(metrics) == 0:\n",
    "        return None\n",
    "    return np.mean(metrics)\n",
    "\n",
    "total = len(df)\n",
    "pbar = tqdm(total=total, desc='Calculating avg_fully_eq_attributes')\n",
    "\n",
    "def apply_avg_fully_eq_attributes(row):\n",
    "    pbar.update(1)\n",
    "    return avg_fully_eq_attributes(\n",
    "        row['characteristic_attributes_mapping_1'], \n",
    "        row['characteristic_attributes_mapping_2']\n",
    "    )\n",
    "\n",
    "df = df.with_columns(\n",
    "    pl.struct(['characteristic_attributes_mapping_1', 'characteristic_attributes_mapping_2'])\n",
    "    .map_elements(apply_avg_fully_eq_attributes, return_dtype=pl.Float64)\n",
    "    .alias('attributes_values_avg_fully_eq')\n",
    ")\n",
    "pbar.close()\n",
    "    \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ebed4a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing name similarity metrics:   0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing name similarity metrics: 100%|██████████| 1000/1000 [00:00<00:00, 6956.30it/s]\n",
      "Processing name_norm similarity metrics: 100%|██████████| 1000/1000 [00:00<00:00, 8115.63it/s]\n",
      "Processing name_en similarity metrics: 100%|██████████| 1000/1000 [00:00<00:00, 33184.10it/s]\n",
      "Processing name_mix similarity metrics: 100%|██████████| 1000/1000 [00:00<00:00, 52720.08it/s]\n",
      "Processing description_en similarity metrics: 100%|██████████| 1000/1000 [00:00<00:00, 5479.21it/s]\n",
      "Processing description_mix similarity metrics: 100%|██████████| 1000/1000 [00:00<00:00, 14618.48it/s]\n",
      "Processing name_tokens_w_digits similarity metrics: 100%|██████████| 1000/1000 [00:00<00:00, 45127.22it/s]\n",
      "Processing description_tokens_w_digits similarity metrics: 100%|██████████| 1000/1000 [00:00<00:00, 7277.24it/s]\n",
      "Processing description similarity metrics: 100%|██████████| 1000/1000 [00:01<00:00, 906.08it/s]\n",
      "Processing description_norm similarity metrics: 100%|██████████| 1000/1000 [00:00<00:00, 1080.21it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# частичные мэтчи по названиям и описаниям\n",
    "\n",
    "def apply_string_metrics(df, col, include_extra_metrics=True):\n",
    "    total = len(df)\n",
    "    pbar = tqdm(total=total, desc=f\"Processing {col} similarity metrics\")\n",
    "    def token_sort_ratio(row):\n",
    "        pbar.update(1)\n",
    "        val1 = row[f'{col}_1']\n",
    "        val2 = row[f'{col}_2']\n",
    "        if val1 is None or val2 is None:\n",
    "            return None\n",
    "        return fuzz.token_sort_ratio(str(val1), str(val2)) / 100\n",
    "    \n",
    "    def token_set_ratio(row):\n",
    "        val1 = row[f'{col}_1']\n",
    "        val2 = row[f'{col}_2']\n",
    "        if val1 is None or val2 is None:\n",
    "            return None\n",
    "        return fuzz.token_set_ratio(str(val1), str(val2)) / 100\n",
    "    \n",
    "    def jaro_winkler_similarity(row):\n",
    "        val1 = row[f'{col}_1']\n",
    "        val2 = row[f'{col}_2']\n",
    "        if val1 is None or val2 is None:\n",
    "            return None\n",
    "        return jellyfish.jaro_winkler_similarity(str(val1), str(val2))\n",
    "    \n",
    "    def dice_similarity(row):\n",
    "        val1 = row[f'{col}_1']\n",
    "        val2 = row[f'{col}_2']\n",
    "        if val1 is None or val2 is None:\n",
    "            return None\n",
    "        return textdistance.dice(str(val1), str(val2))\n",
    "    \n",
    "    def tanimoto_similarity(row):\n",
    "        val1 = row[f'{col}_1']\n",
    "        val2 = row[f'{col}_2']\n",
    "        if val1 is None or val2 is None:\n",
    "            return None\n",
    "        return textdistance.tanimoto(str(val1), str(val2))\n",
    "    \n",
    "    def sorensen_similarity(row):\n",
    "        val1 = row[f'{col}_1']\n",
    "        val2 = row[f'{col}_2']\n",
    "        if val1 is None or val2 is None:\n",
    "            return None\n",
    "        return textdistance.sorensen(str(val1), str(val2))\n",
    "    \n",
    "    new_columns = [\n",
    "        pl.struct([f'{col}_1', f'{col}_2']).map_elements(\n",
    "            token_sort_ratio, return_dtype=pl.Float64\n",
    "        ).alias(f'{col}_token_sort_ratio'),\n",
    "        \n",
    "        pl.struct([f'{col}_1', f'{col}_2']).map_elements(\n",
    "            token_set_ratio, return_dtype=pl.Float64\n",
    "        ).alias(f'{col}_token_set_ratio'),\n",
    "        \n",
    "        pl.struct([f'{col}_1', f'{col}_2']).map_elements(\n",
    "            jaro_winkler_similarity, return_dtype=pl.Float64\n",
    "        ).alias(f'{col}_jaro_winkler_similarity'),\n",
    "        \n",
    "        pl.struct([f'{col}_1', f'{col}_2']).map_elements(\n",
    "            dice_similarity, return_dtype=pl.Float64\n",
    "        ).alias(f'{col}_dice'),\n",
    "        \n",
    "        pl.struct([f'{col}_1', f'{col}_2']).map_elements(\n",
    "            tanimoto_similarity, return_dtype=pl.Float64\n",
    "        ).alias(f'{col}_tanimoto'),\n",
    "        \n",
    "        pl.struct([f'{col}_1', f'{col}_2']).map_elements(\n",
    "            sorensen_similarity, return_dtype=pl.Float64\n",
    "        ).alias(f'{col}_sorensen')\n",
    "    ]\n",
    "    \n",
    "    if include_extra_metrics and 'attr' not in col:\n",
    "        def damerau_levenshtein_distance(row):\n",
    "            val1 = row[f'{col}_1']\n",
    "            val2 = row[f'{col}_2']\n",
    "            if val1 is None or val2 is None:\n",
    "                return None\n",
    "            return jellyfish.damerau_levenshtein_distance(str(val1), str(val2))\n",
    "        \n",
    "        def wratio_similarity(row):\n",
    "            val1 = row[f'{col}_1']\n",
    "            val2 = row[f'{col}_2']\n",
    "            if val1 is None or val2 is None:\n",
    "                return None\n",
    "            return fuzz.WRatio(str(val1), str(val2)) / 100\n",
    "        \n",
    "        new_columns.extend([\n",
    "            pl.struct([f'{col}_1', f'{col}_2']).map_elements(\n",
    "                damerau_levenshtein_distance, return_dtype=pl.Int64\n",
    "            ).alias(f'{col}_damerau_levenshtein_distance'),\n",
    "            \n",
    "            pl.struct([f'{col}_1', f'{col}_2']).map_elements(\n",
    "                wratio_similarity, return_dtype=pl.Float64\n",
    "            ).alias(f'{col}_WRatio')\n",
    "        ])\n",
    "    \n",
    "    df = df.with_columns(new_columns)\n",
    "    pbar.close()\n",
    "    return df\n",
    "\n",
    "def apply_string_metrics_desc(df, col):\n",
    "    total = len(df)\n",
    "    pbar = tqdm(total=total, desc=f\"Processing {col} similarity metrics\")\n",
    "    def token_sort_ratio(row):\n",
    "        pbar.update(1)\n",
    "        val1 = row[f'{col}_1']\n",
    "        val2 = row[f'{col}_2']\n",
    "        if val1 is None or val2 is None:\n",
    "            return None\n",
    "        return fuzz.token_sort_ratio(str(val1), str(val2)) / 100\n",
    "    \n",
    "    def token_set_ratio(row):\n",
    "        val1 = row[f'{col}_1']\n",
    "        val2 = row[f'{col}_2']\n",
    "        if val1 is None or val2 is None:\n",
    "            return None\n",
    "        return fuzz.token_set_ratio(str(val1), str(val2)) / 100\n",
    "    \n",
    "    def jaro_winkler_similarity(row):\n",
    "        val1 = row[f'{col}_1']\n",
    "        val2 = row[f'{col}_2']\n",
    "        if val1 is None or val2 is None:\n",
    "            return None\n",
    "        return jellyfish.jaro_winkler_similarity(str(val1), str(val2))\n",
    "    \n",
    "    def dice_similarity(row):\n",
    "        val1 = row[f'{col}_1']\n",
    "        val2 = row[f'{col}_2']\n",
    "        if val1 is None or val2 is None:\n",
    "            return None\n",
    "        return textdistance.dice(str(val1), str(val2))\n",
    "    \n",
    "    new_columns = [\n",
    "        pl.struct([f'{col}_1', f'{col}_2']).map_elements(\n",
    "            token_sort_ratio, return_dtype=pl.Float64\n",
    "        ).alias(f'{col}_token_sort_ratio'),\n",
    "        \n",
    "        pl.struct([f'{col}_1', f'{col}_2']).map_elements(\n",
    "            token_set_ratio, return_dtype=pl.Float64\n",
    "        ).alias(f'{col}_token_set_ratio'),\n",
    "        \n",
    "        pl.struct([f'{col}_1', f'{col}_2']).map_elements(\n",
    "            jaro_winkler_similarity, return_dtype=pl.Float64\n",
    "        ).alias(f'{col}_jaro_winkler_similarity'),\n",
    "        \n",
    "        pl.struct([f'{col}_1', f'{col}_2']).map_elements(\n",
    "            dice_similarity, return_dtype=pl.Float64\n",
    "        ).alias(f'{col}_dice'),\n",
    "    ]\n",
    "    \n",
    "    df = df.with_columns(new_columns)\n",
    "    pbar.close()\n",
    "    return df\n",
    "\n",
    "for col in ('name', 'name_norm', 'name_en', 'name_mix', 'description_en', 'description_mix', 'name_tokens_w_digits', 'description_tokens_w_digits'):\n",
    "    df = apply_string_metrics(df, col)\n",
    "for col in ('description', 'description_norm'):\n",
    "    df = apply_string_metrics_desc(df, col)\n",
    "    \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66e533f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating top attribute matches for level 1: 100%|██████████| 1000/1000 [00:00<00:00, 24448.46it/s]\n",
      "Generating top attribute matches for level 2: 100%|██████████| 1000/1000 [00:00<00:00, 30880.21it/s]\n",
      "Generating top attribute matches for level 3: 100%|██████████| 1000/1000 [00:00<00:00, 32723.26it/s]\n",
      "Generating top attribute matches for level 4:   0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating top attribute matches for level 4: 100%|██████████| 1000/1000 [00:00<00:00, 42223.83it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# совпадения топ-атрибутов по категориям\n",
    "\n",
    "pop_characts_tf_idf = {}\n",
    "for level in range(1, 5):\n",
    "    with open(f'../data/preprocessed/pop_characts_tf_idf_level_{level}.pkl', 'rb') as file:\n",
    "        pop_characts_tf_idf[level] = pickle.load(file)\n",
    "\n",
    "def generate_top_attribute_matches(df, level):\n",
    "    if level == 1:\n",
    "        TOP_N_characts = 75\n",
    "    elif level == 2:\n",
    "        TOP_N_characts = 50\n",
    "    elif level == 3:\n",
    "        TOP_N_characts = 50\n",
    "    elif level == 4:\n",
    "        TOP_N_characts = 25\n",
    "    else:\n",
    "        print('u are gay')\n",
    "\n",
    "    total = len(df)\n",
    "    pbar = tqdm(total=total, desc=f\"Generating top attribute matches for level {level}\")\n",
    "    \n",
    "    def code_top_tf_idf_characteristics(row):\n",
    "        pbar.update(1)\n",
    "        cat_1 = row[f'category_level_{level}_1']\n",
    "        cat_2 = row[f'category_level_{level}_2']\n",
    "        attr_1 = row['characteristic_attributes_mapping_1']\n",
    "        attr_2 = row['characteristic_attributes_mapping_2']\n",
    "        \n",
    "        result = [0.0] * TOP_N_characts\n",
    "        \n",
    "        if attr_1 is None or attr_2 is None or cat_1 != cat_2 or cat_1 not in pop_characts_tf_idf[level]:\n",
    "            return result\n",
    "        \n",
    "        try:\n",
    "            attr_1 = json.loads(attr_1)\n",
    "            attr_2 = json.loads(attr_2)\n",
    "        except (json.JSONDecodeError, TypeError):\n",
    "            print('bad!!!')\n",
    "            return result\n",
    "        \n",
    "        top_pop_characts = [i[1] for i in pop_characts_tf_idf[level][cat_1][:TOP_N_characts]]\n",
    "        \n",
    "        for i, cat_name in enumerate(top_pop_characts):\n",
    "            if i >= TOP_N_characts:\n",
    "                break\n",
    "            if cat_name in attr_1 and cat_name in attr_2:\n",
    "                if attr_1[cat_name] == attr_2[cat_name]:\n",
    "                    result[i] = 1.0\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    df = df.with_columns(\n",
    "        pl.struct([\n",
    "            f'category_level_{level}_1', \n",
    "            f'category_level_{level}_2', \n",
    "            'characteristic_attributes_mapping_1', \n",
    "            'characteristic_attributes_mapping_2'\n",
    "        ]).map_elements(code_top_tf_idf_characteristics, return_dtype=pl.List(pl.Float64))\n",
    "        .alias(f'top_attr_match_list_lvl{level}')\n",
    "    )\n",
    "    \n",
    "    new_cols = []\n",
    "    new_col_names = []\n",
    "    for i in range(TOP_N_characts):\n",
    "        col_name = f'top_{i}_attr_match_lvl{level}'\n",
    "        new_cols.append(\n",
    "            pl.col(f'top_attr_match_list_lvl{level}').list.get(i).alias(col_name)\n",
    "        )\n",
    "        new_col_names.append(col_name)\n",
    "    \n",
    "    df = df.with_columns(new_cols)\n",
    "    \n",
    "    df = df.with_columns(\n",
    "        pl.sum_horizontal([pl.col(c).cast(pl.Float64) for c in new_col_names])\n",
    "        .alias(f'total_top_attribute_matches_lvl{level}')\n",
    "    )\n",
    "    \n",
    "    df = df.drop(f'top_attr_match_list_lvl{level}')\n",
    "    pbar.close()\n",
    "    \n",
    "    return df\n",
    "\n",
    "for level in range(1, 5):\n",
    "    df = generate_top_attribute_matches(df, level)\n",
    "    gc.collect() \n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d8936d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating weighted attribute matches for level 1: 100%|██████████| 1000/1000 [00:00<00:00, 5690.70it/s]\n",
      "Generating weighted attribute matches for level 2: 100%|██████████| 1000/1000 [00:00<00:00, 11058.92it/s]\n",
      "Generating weighted attribute matches for level 3: 100%|██████████| 1000/1000 [00:00<00:00, 19062.59it/s]\n",
      "Generating weighted attribute matches for level 4: 100%|██████████| 1000/1000 [00:00<00:00, 34916.45it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# взвешенное совпадение атрибутов\n",
    "\n",
    "def generate_weighted_attribute_matches(df, level):\n",
    "    if level == 1:\n",
    "        n_values = [10, 25, 50, 75, 100, 150, 200, 250, 500]\n",
    "    elif level == 2:\n",
    "        n_values = [10, 25, 50, 75, 100, 150, 200]\n",
    "    elif level == 3:\n",
    "        n_values = [10, 25, 50, 75, 100]\n",
    "    elif level == 4:\n",
    "        n_values = [10, 25, 50]\n",
    "    else:\n",
    "        print('u are gay')\n",
    "    \n",
    "    total = len(df)\n",
    "    pbar = tqdm(total=total, desc=f\"Generating weighted attribute matches for level {level}\")\n",
    "    \n",
    "    def code_weighted_tf_idf_characteristics(row):\n",
    "        pbar.update(1)\n",
    "        cat_1 = row[f'category_level_{level}_1']\n",
    "        cat_2 = row[f'category_level_{level}_2']\n",
    "        attr_1 = row['characteristic_attributes_mapping_1']\n",
    "        attr_2 = row['characteristic_attributes_mapping_2']\n",
    "        \n",
    "        result = {n: 0.0 for n in n_values}\n",
    "        \n",
    "        if attr_1 is None or attr_2 is None or cat_1 != cat_2 or cat_1 not in pop_characts_tf_idf[level]:\n",
    "            return [result[n] for n in n_values]\n",
    "        \n",
    "        try:\n",
    "            attr_1 = json.loads(attr_1)\n",
    "            attr_2 = json.loads(attr_2)\n",
    "        except (json.JSONDecodeError, TypeError):\n",
    "            return [result[n] for n in n_values]\n",
    "        \n",
    "        top_pop_characts = pop_characts_tf_idf[level][cat_1]\n",
    "        \n",
    "        for n in n_values:\n",
    "            limit = min(n, len(top_pop_characts))\n",
    "            weighted_sum = 0.0\n",
    "            total_weight = 0.0\n",
    "            for i in range(limit):\n",
    "                weight, cat_name = top_pop_characts[i]\n",
    "                total_weight += weight\n",
    "                if cat_name in attr_1 and cat_name in attr_2 and attr_1[cat_name] == attr_2[cat_name]:\n",
    "                    weighted_sum += weight\n",
    "            if total_weight > 0:\n",
    "                result[n] = weighted_sum / total_weight\n",
    "            else:\n",
    "                result[n] = 0.0\n",
    "        \n",
    "        return [result[n] for n in n_values]\n",
    "    \n",
    "    df = df.with_columns(\n",
    "        pl.struct([\n",
    "            f'category_level_{level}_1', \n",
    "            f'category_level_{level}_2', \n",
    "            'characteristic_attributes_mapping_1', \n",
    "            'characteristic_attributes_mapping_2'\n",
    "        ]).map_elements(code_weighted_tf_idf_characteristics, return_dtype=pl.List(pl.Float64))\n",
    "        .alias(f'weighted_top_attr_match_list_lvl{level}')\n",
    "    )\n",
    "    \n",
    "    new_cols = []\n",
    "    new_col_names = []\n",
    "    for n in n_values:\n",
    "        col_name = f'weighted_top_attr_match_lvl{level}_n{n}'\n",
    "        new_cols.append(\n",
    "            pl.col(f'weighted_top_attr_match_list_lvl{level}').list.get(n_values.index(n)).alias(col_name)\n",
    "        )\n",
    "        new_col_names.append(col_name)\n",
    "    \n",
    "    df = df.with_columns(new_cols)\n",
    "    \n",
    "    df = df.drop(f'weighted_top_attr_match_list_lvl{level}')\n",
    "    pbar.close()\n",
    "    \n",
    "    return df\n",
    "\n",
    "for level in range(1, 5):\n",
    "    df = generate_weighted_attribute_matches(df, level)\n",
    "    gc.collect() \n",
    "    \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddc9828",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating LCP and LCS for name_norm: 100%|██████████| 1000/1000 [00:00<00:00, 2864.42it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lcp&lcs для названий\n",
    "\n",
    "def longest_common_prefix(str1, str2):\n",
    "    if str1 is None or str2 is None:\n",
    "        return None\n",
    "    \n",
    "    min_len = min(len(str1), len(str2))\n",
    "    prefix_len = 0\n",
    "    \n",
    "    for i in range(min_len):\n",
    "        if str1[i] == str2[i]:\n",
    "            prefix_len += 1\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return prefix_len / min_len if min_len != 0 else 0\n",
    "\n",
    "def longest_common_subsequence(str1, str2):\n",
    "    if str1 is None or str2 is None:\n",
    "        return None\n",
    "    \n",
    "    len1, len2 = len(str1), len(str2)\n",
    "    dp = [[0] * (len2 + 1) for _ in range(len1 + 1)]\n",
    "    \n",
    "    for i in range(1, len1 + 1):\n",
    "        for j in range(1, len2 + 1):\n",
    "            if str1[i - 1] == str2[j - 1]:\n",
    "                dp[i][j] = dp[i - 1][j - 1] + 1\n",
    "            else:\n",
    "                dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])\n",
    "    \n",
    "    lcs_len = dp[len1][len2]\n",
    "    return lcs_len / max(len1, len2) if max(len1, len2) != 0 else 0\n",
    "\n",
    "for col in ('name_norm',):\n",
    "    total = len(df) * 2\n",
    "    pbar = tqdm(total=total, desc=f'Calculating LCP and LCS for {col}')\n",
    "    \n",
    "    def lcp_with_progress(row):\n",
    "        pbar.update(1)\n",
    "        return longest_common_prefix(row[f'{col}_1'], row[f'{col}_2'])\n",
    "    \n",
    "    def lcs_with_progress(row):\n",
    "        pbar.update(1)\n",
    "        return longest_common_subsequence(row[f'{col}_1'], row[f'{col}_2'])\n",
    "    \n",
    "    df = df.with_columns([\n",
    "        pl.struct([f'{col}_1', f'{col}_2'])\n",
    "        .map_elements(lcp_with_progress, return_dtype=pl.Float64)\n",
    "        .alias(f'{col}_lcp'),\n",
    "        \n",
    "        pl.struct([f'{col}_1', f'{col}_2'])\n",
    "        .map_elements(lcs_with_progress, return_dtype=pl.Float64)\n",
    "        .alias(f'{col}_lcs')\n",
    "    ])\n",
    "    pbar.close()\n",
    "    \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "db55ae6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing description_tokens jaccard and overlap scores: 100%|██████████| 1000/1000 [00:00<00:00, 9425.47it/s]\n",
      "Processing description_en_tokens jaccard and overlap scores: 100%|██████████| 1000/1000 [00:00<00:00, 131561.24it/s]\n",
      "Processing description_mix_tokens jaccard and overlap scores: 100%|██████████| 1000/1000 [00:00<00:00, 155068.91it/s]\n",
      "Processing name_tokens jaccard and overlap scores: 100%|██████████| 1000/1000 [00:00<00:00, 113270.79it/s]\n",
      "Processing name_en_tokens jaccard and overlap scores: 100%|██████████| 1000/1000 [00:00<00:00, 197537.04it/s]\n",
      "Processing name_mix_tokens jaccard and overlap scores: 100%|██████████| 1000/1000 [00:00<00:00, 221230.23it/s]\n",
      "Processing description_tokens_w_digits jaccard and overlap scores: 100%|██████████| 1000/1000 [00:00<00:00, 129150.88it/s]\n",
      "Processing name_tokens_w_digits jaccard and overlap scores: 100%|██████████| 1000/1000 [00:00<00:00, 228622.26it/s]\n",
      "Processing attr_keys jaccard and overlap scores: 100%|██████████| 1000/1000 [00:00<00:00, 105487.89it/s]\n",
      "Processing attr_vals jaccard and overlap scores: 100%|██████████| 1000/1000 [00:00<00:00, 96774.51it/s]\n",
      "Processing units_name jaccard and overlap scores: 100%|██████████| 1000/1000 [00:00<00:00, 201862.74it/s]\n",
      "Processing units_desc jaccard and overlap scores: 100%|██████████| 1000/1000 [00:00<00:00, 157178.34it/s]\n",
      "Processing brands_name jaccard and overlap scores: 100%|██████████| 1000/1000 [00:00<00:00, 187379.56it/s]\n",
      "Processing brands_desc jaccard and overlap scores: 100%|██████████| 1000/1000 [00:00<00:00, 129746.16it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# сходство для списков\n",
    "\n",
    "def jaccard_similarity(list1, list2):\n",
    "    if list1 is None or list2 is None:\n",
    "        return None\n",
    "    set1 = set(list1)\n",
    "    set2 = set(list2)\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    return intersection / union if union != 0 else 0\n",
    "\n",
    "def overlap_coefficient(list1, list2):\n",
    "    if list1 is None or list2 is None:\n",
    "        return None\n",
    "    set1 = set(list1)\n",
    "    set2 = set(list2)\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    return intersection / min(len(set1), len(set2)) if min(len(set1), len(set2)) != 0 else 0\n",
    "\n",
    "token_cols = [\n",
    "    'description_tokens', \n",
    "    'description_en_tokens',\n",
    "    'description_mix_tokens',\n",
    "    'name_tokens', \n",
    "    'name_en_tokens',\n",
    "    'name_mix_tokens',\n",
    "    'description_tokens_w_digits', \n",
    "    'name_tokens_w_digits'\n",
    "]\n",
    "\n",
    "collection_cols = [\n",
    "    'attr_keys', \n",
    "    'attr_vals', \n",
    "    'units_name',\n",
    "    'units_desc',\n",
    "    'brands_name',\n",
    "    'brands_desc',\n",
    "    # 'colors_name',\n",
    "    # 'colors_desc'\n",
    "]\n",
    "\n",
    "for col in token_cols:\n",
    "    total = len(df)\n",
    "    pbar = tqdm(total=total, desc=f\"Processing {col} jaccard and overlap scores\")\n",
    "\n",
    "    def jaccard_score(row):\n",
    "        pbar.update(1)\n",
    "        val1 = row[f'{col}_1']\n",
    "        val2 = row[f'{col}_2']\n",
    "        if val1 is None or val2 is None:\n",
    "            return None\n",
    "        try:\n",
    "            list1 = val1.split()\n",
    "            list2 = val2.split()\n",
    "            return jaccard_similarity(list1, list2)\n",
    "        except (AttributeError, TypeError):\n",
    "            print('bad!!!')\n",
    "            return None\n",
    "\n",
    "    def overlap_score(row):\n",
    "        val1 = row[f'{col}_1']\n",
    "        val2 = row[f'{col}_2']\n",
    "        if val1 is None or val2 is None:\n",
    "            return None\n",
    "        try:\n",
    "            list1 = val1.split()\n",
    "            list2 = val2.split()\n",
    "            return overlap_coefficient(list1, list2)\n",
    "        except (AttributeError, TypeError):\n",
    "            print('bad!!!')\n",
    "            return None\n",
    "\n",
    "    df = df.with_columns(\n",
    "        pl.struct([f'{col}_1', f'{col}_2']).map_elements(jaccard_score, return_dtype=pl.Float64).alias(f'{col}_jaccard_score'),\n",
    "        pl.struct([f'{col}_1', f'{col}_2']).map_elements(overlap_score, return_dtype=pl.Float64).alias(f'{col}_overlap_score')\n",
    "    )\n",
    "    pbar.close()\n",
    "\n",
    "for col in collection_cols:\n",
    "    total = len(df)\n",
    "    pbar = tqdm(total=total, desc=f\"Processing {col} jaccard and overlap scores\")\n",
    "\n",
    "    def jaccard_score_collection(row):\n",
    "        pbar.update(1)\n",
    "        val1 = row[f'{col}_1']\n",
    "        val2 = row[f'{col}_2']\n",
    "        if val1 is None or val2 is None:\n",
    "            return None\n",
    "        try:\n",
    "            return jaccard_similarity(val1, val2)\n",
    "        except Exception:\n",
    "            print('bad!!!')\n",
    "            return None\n",
    "\n",
    "    def overlap_score_collection(row):\n",
    "        val1 = row[f'{col}_1']\n",
    "        val2 = row[f'{col}_2']\n",
    "        if val1 is None or val2 is None:\n",
    "            return None\n",
    "        try:\n",
    "            return overlap_coefficient(val1, val2)\n",
    "        except Exception:\n",
    "            print('bad!!!')\n",
    "            return None\n",
    "\n",
    "    df = df.with_columns(\n",
    "        pl.struct([f'{col}_1', f'{col}_2']).map_elements(jaccard_score_collection, return_dtype=pl.Float64).alias(f'{col}_jaccard_score'),\n",
    "        pl.struct([f'{col}_1', f'{col}_2']).map_elements(overlap_score_collection, return_dtype=pl.Float64).alias(f'{col}_overlap_score')\n",
    "    )\n",
    "    pbar.close()\n",
    "    \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c6394f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing attr_keys common elements count: 100%|██████████| 1000/1000 [00:00<00:00, 197574.26it/s]\n",
      "Processing attr_vals common elements count: 100%|██████████| 1000/1000 [00:00<00:00, 200540.47it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# количество совпадющих ключей и значений атрибутов\n",
    "\n",
    "collection_cols_for_common_count = ['attr_keys', 'attr_vals',]\n",
    "\n",
    "for col in collection_cols_for_common_count:\n",
    "    total = len(df)\n",
    "    pbar = tqdm(total=total, desc=f\"Processing {col} common elements count\")\n",
    "\n",
    "    def common_elements_count(row):\n",
    "        pbar.update(1)\n",
    "        val1 = row[f'{col}_1']\n",
    "        val2 = row[f'{col}_2']\n",
    "        if val1 is None or val2 is None:\n",
    "            return None\n",
    "        try:\n",
    "            set1 = set(val1)\n",
    "            set2 = set(val2)\n",
    "            return len(set1.intersection(set2))\n",
    "        except Exception:\n",
    "            print('bad!!!')\n",
    "            return None\n",
    "\n",
    "    df = df.with_columns(\n",
    "        pl.struct([f'{col}_1', f'{col}_2']).map_elements(common_elements_count, return_dtype=pl.Int64).alias(f'{col}_common_count')\n",
    "    )\n",
    "    pbar.close()\n",
    "    \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2818989d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating BM25Okapi name scores: 100%|██████████| 2000/2000 [00:00<00:00, 16254.63it/s]\n",
      "Calculating BM25Okapi scores: 100%|██████████| 2000/2000 [00:02<00:00, 826.13it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bm25 на name+desc\n",
    "\n",
    "def tokenize(text):\n",
    "    if text is None:\n",
    "        return []\n",
    "    return re.findall(r'\\w+', str(text).lower())\n",
    "\n",
    "def calculate_bm25_score_name(df):\n",
    "    total = len(df)\n",
    "    pbar = tqdm(total=total*2, desc='Calculating BM25Okapi name scores')\n",
    "    \n",
    "    def bm25_score_left(row):\n",
    "        pbar.update(1)\n",
    "        \n",
    "        name_1 = row['name_1']\n",
    "        name_2 = row['name_2']\n",
    "        \n",
    "        name_1 = \"\" if name_1 is None else str(name_1)\n",
    "        name_2 = \"\" if name_2 is None else str(name_2)\n",
    "        \n",
    "        tokens_1 = tokenize(name_1)\n",
    "        tokens_2 = tokenize(name_2)\n",
    "        \n",
    "        if not tokens_1 or not tokens_2:\n",
    "            return None\n",
    "        \n",
    "        bm25 = BM25Okapi([tokens_2])\n",
    "        scores = bm25.get_scores(tokens_1)\n",
    "        \n",
    "        return float(scores[0]) if len(scores) > 0 else None\n",
    "    \n",
    "    def bm25_score_right(row):\n",
    "        pbar.update(1)\n",
    "        \n",
    "        name_1 = row['name_2']\n",
    "        name_2 = row['name_1']\n",
    "        \n",
    "        name_1 = \"\" if name_1 is None else str(name_1)\n",
    "        name_2 = \"\" if name_2 is None else str(name_2)\n",
    "        \n",
    "        tokens_1 = tokenize(name_1)\n",
    "        tokens_2 = tokenize(name_2)\n",
    "        \n",
    "        if not tokens_1 or not tokens_2:\n",
    "            return None\n",
    "        \n",
    "        bm25 = BM25Okapi([tokens_2])\n",
    "        scores = bm25.get_scores(tokens_1)\n",
    "        \n",
    "        return float(scores[0]) if len(scores) > 0 else None\n",
    "    \n",
    "    df = df.with_columns([\n",
    "        pl.struct(['name_1', 'name_2'])\n",
    "        .map_elements(bm25_score_left, return_dtype=pl.Float64)\n",
    "        .alias('bm25_name_score_left'),\n",
    "        pl.struct(['name_1', 'name_2'])\n",
    "        .map_elements(bm25_score_right, return_dtype=pl.Float64)\n",
    "        .alias('bm25_name_score_right')\n",
    "    ])\n",
    "    pbar.close()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def calculate_bm25_score(df):\n",
    "    total = len(df)\n",
    "    pbar = tqdm(total=total*2, desc='Calculating BM25Okapi scores')\n",
    "    \n",
    "    def bm25_score_left(row):\n",
    "        pbar.update(1)\n",
    "        \n",
    "        name_1 = row['name_1']\n",
    "        desc_1 = row['description_1']\n",
    "        name_2 = row['name_2']\n",
    "        desc_2 = row['description_2']\n",
    "        \n",
    "        name_1 = \"\" if name_1 is None else str(name_1)\n",
    "        desc_1 = \"\" if desc_1 is None else str(desc_1)\n",
    "        name_2 = \"\" if name_2 is None else str(name_2)\n",
    "        desc_2 = \"\" if desc_2 is None else str(desc_2)\n",
    "        \n",
    "        combined_1 = f\"Name: {name_1}, Desc: {desc_1}\"\n",
    "        combined_2 = f\"Name: {name_2}, Desc: {desc_2}\"\n",
    "        \n",
    "        tokens_1 = tokenize(combined_1)\n",
    "        tokens_2 = tokenize(combined_2)\n",
    "        \n",
    "        if not tokens_1 or not tokens_2:\n",
    "            return None\n",
    "        \n",
    "        bm25 = BM25Okapi([tokens_2])\n",
    "        scores = bm25.get_scores(tokens_1)\n",
    "        \n",
    "        return float(scores[0]) if len(scores) > 0 else None\n",
    "    \n",
    "    def bm25_score_right(row):\n",
    "        pbar.update(1)\n",
    "        \n",
    "        name_1 = row['name_2']\n",
    "        desc_1 = row['description_2']\n",
    "        name_2 = row['name_1']\n",
    "        desc_2 = row['description_1']\n",
    "        \n",
    "        name_1 = \"\" if name_1 is None else str(name_1)\n",
    "        desc_1 = \"\" if desc_1 is None else str(desc_1)\n",
    "        name_2 = \"\" if name_2 is None else str(name_2)\n",
    "        desc_2 = \"\" if desc_2 is None else str(desc_2)\n",
    "        \n",
    "        combined_1 = f\"Name: {name_1}, Desc: {desc_1}\"\n",
    "        combined_2 = f\"Name: {name_2}, Desc: {desc_2}\"\n",
    "        \n",
    "        tokens_1 = tokenize(combined_1)\n",
    "        tokens_2 = tokenize(combined_2)\n",
    "        \n",
    "        if not tokens_1 or not tokens_2:\n",
    "            return None\n",
    "        \n",
    "        bm25 = BM25Okapi([tokens_2])\n",
    "        scores = bm25.get_scores(tokens_1)\n",
    "        \n",
    "        return float(scores[0]) if len(scores) > 0 else None\n",
    "    \n",
    "    df = df.with_columns([\n",
    "        pl.struct(['name_1', 'description_1', 'name_2', 'description_2'])\n",
    "        .map_elements(bm25_score_left, return_dtype=pl.Float64)\n",
    "        .alias('bm25_name_desc_score_left'),\n",
    "        pl.struct(['name_1', 'description_1', 'name_2', 'description_2'])\n",
    "        .map_elements(bm25_score_right, return_dtype=pl.Float64)\n",
    "        .alias('bm25_name_desc_score_right')\n",
    "    ])\n",
    "    pbar.close()\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = calculate_bm25_score_name(df)\n",
    "gc.collect()\n",
    "\n",
    "df = calculate_bm25_score(df)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "55aa2ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating BM25Okapi keyval scores: 100%|██████████| 2000/2000 [00:00<00:00, 10569.34it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bm25 по key=val атрибутам\n",
    "\n",
    "def tokenize_keyval(text):\n",
    "    if text is None:\n",
    "        return []\n",
    "    return str(text).strip().split()\n",
    "\n",
    "def calculate_bm25_score_keyval(df):\n",
    "    total = len(df)\n",
    "    pbar = tqdm(total=total*2, desc='Calculating BM25Okapi keyval scores')\n",
    "\n",
    "    def bm25_score_left(row):\n",
    "        pbar.update(1)\n",
    "        keyval_1 = row['concat_keyval_1']\n",
    "        keyval_2 = row['concat_keyval_2']\n",
    "\n",
    "        tokens_1 = tokenize_keyval(keyval_1)\n",
    "        tokens_2 = tokenize_keyval(keyval_2)\n",
    "\n",
    "        if not tokens_1 or not tokens_2:\n",
    "            return None\n",
    "\n",
    "        bm25 = BM25Okapi([tokens_2])\n",
    "        scores = bm25.get_scores(tokens_1)\n",
    "        return float(scores[0]) if len(scores) > 0 else None\n",
    "\n",
    "    def bm25_score_right(row):\n",
    "        pbar.update(1)\n",
    "        keyval_1 = row['concat_keyval_2']\n",
    "        keyval_2 = row['concat_keyval_1']\n",
    "\n",
    "        tokens_1 = tokenize_keyval(keyval_1)\n",
    "        tokens_2 = tokenize_keyval(keyval_2)\n",
    "\n",
    "        if not tokens_1 or not tokens_2:\n",
    "            return None\n",
    "\n",
    "        bm25 = BM25Okapi([tokens_2])\n",
    "        scores = bm25.get_scores(tokens_1)\n",
    "        return float(scores[0]) if len(scores) > 0 else None\n",
    "\n",
    "    df = df.with_columns([\n",
    "        pl.struct(['concat_keyval_1', 'concat_keyval_2'])\n",
    "        .map_elements(bm25_score_left, return_dtype=pl.Float64)\n",
    "        .alias('bm25_keyval_score_left'),\n",
    "        pl.struct(['concat_keyval_1', 'concat_keyval_2'])\n",
    "        .map_elements(bm25_score_right, return_dtype=pl.Float64)\n",
    "        .alias('bm25_keyval_score_right')\n",
    "    ])\n",
    "    pbar.close()\n",
    "    return df\n",
    "\n",
    "df = calculate_bm25_score_keyval(df)\n",
    "    \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "91014210",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating n-gram Similarities for multiple columns: 100%|██████████| 8000/8000 [00:02<00:00, 2731.07it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# iou по n-gram\n",
    "\n",
    "def calculate_ngram_similarities_multiple_cols(df, cols):\n",
    "    total = len(df) * len(cols)\n",
    "    pbar = tqdm(total=total, desc='Calculating n-gram Similarities for multiple columns')\n",
    "    \n",
    "    def get_ngrams(text, n):\n",
    "        text = text.lower()\n",
    "        return [text[i:i+n] for i in range(len(text)-n+1)]\n",
    "    \n",
    "    def ngram_similarities(row, col):\n",
    "        pbar.update(1)\n",
    "        val_1 = \"\" if row[f'{col}_1'] is None else str(row[f'{col}_1'])\n",
    "        val_2 = \"\" if row[f'{col}_2'] is None else str(row[f'{col}_2'])\n",
    "        \n",
    "        sims = []\n",
    "        for n in [1, 2, 3, 4, 5, 6, 7]:\n",
    "            ngrams_1 = set(get_ngrams(val_1, n)) if len(val_1) >= n else set()\n",
    "            ngrams_2 = set(get_ngrams(val_2, n)) if len(val_2) >= n else set()\n",
    "            sim = 0.0\n",
    "            if ngrams_1 and ngrams_2:\n",
    "                intersection = len(ngrams_1 & ngrams_2)\n",
    "                union = len(ngrams_1 | ngrams_2)\n",
    "                sim = intersection / union if union > 0 else 0.0\n",
    "            sims.append(sim)\n",
    "        return sims\n",
    "    \n",
    "    for col in cols:\n",
    "        df = df.with_columns(\n",
    "            pl.struct([f'{col}_1', f'{col}_2'])\n",
    "            .map_elements(lambda row: ngram_similarities(row, col), return_dtype=pl.List(pl.Float64))\n",
    "            .alias(f'ngram_similarities_{col}')\n",
    "        )\n",
    "    \n",
    "    for col in cols:\n",
    "        df = df.with_columns([\n",
    "            pl.col(f'ngram_similarities_{col}').list.get(0).alias(f'{col}_char_1gram_iou'),\n",
    "            pl.col(f'ngram_similarities_{col}').list.get(1).alias(f'{col}_char_2gram_iou'),\n",
    "            pl.col(f'ngram_similarities_{col}').list.get(2).alias(f'{col}_char_3gram_iou'),\n",
    "            pl.col(f'ngram_similarities_{col}').list.get(3).alias(f'{col}_char_4gram_iou'),\n",
    "            pl.col(f'ngram_similarities_{col}').list.get(4).alias(f'{col}_char_5gram_iou'),\n",
    "            pl.col(f'ngram_similarities_{col}').list.get(5).alias(f'{col}_char_6gram_iou'),\n",
    "            pl.col(f'ngram_similarities_{col}').list.get(6).alias(f'{col}_char_7gram_iou'),\n",
    "        ]).drop(f'ngram_similarities_{col}')\n",
    "    \n",
    "    pbar.close()\n",
    "    return df\n",
    "\n",
    "columns_ngram = [\n",
    "    'name',\n",
    "    'name_en', \n",
    "    'name_mix',\n",
    "    'name_tokens_w_digits',\n",
    "    'description',\n",
    "    'description_en', \n",
    "    'description_mix', \n",
    "    'description_tokens_w_digits'\n",
    "]\n",
    "\n",
    "df = calculate_ngram_similarities_multiple_cols(df, columns_ngram)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eee607c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating ROUGE Metrics: 100%|██████████| 1000/1000 [00:00<00:00, 10892.68it/s]\n",
      "Calculating ROUGE Metrics: 100%|██████████| 1000/1000 [00:19<00:00, 51.99it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rouge по name+desc, очень долго\n",
    "\n",
    "def calculate_rouge_metrics(df, col):\n",
    "    total = len(df)\n",
    "    pbar = tqdm(total=total, desc='Calculating ROUGE Metrics')\n",
    "    \n",
    "    def rouge_scores(row):\n",
    "        pbar.update(1)\n",
    "        col_1 = \"\" if row[f'{col}_1'] is None else str(row[f'{col}_1'])\n",
    "        col_2 = \"\" if row[f'{col}_2'] is None else str(row[f'{col}_2'])\n",
    "        \n",
    "        if len(col_1) < 1 or len(col_2) < 1:\n",
    "            return [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "        \n",
    "        try:\n",
    "            rouge = Rouge()\n",
    "            scores = rouge.get_scores(col_1, col_2)[0]\n",
    "            \n",
    "            rouge1_f = scores['rouge-1']['f']\n",
    "            rouge1_p = scores['rouge-1']['p']\n",
    "            rouge1_r = scores['rouge-1']['r']\n",
    "            \n",
    "            rouge2_f = scores['rouge-2']['f']\n",
    "            \n",
    "            rougeL_f = scores['rouge-l']['f']\n",
    "            rougeL_r = scores['rouge-l']['r']\n",
    "            \n",
    "            return [rouge1_f, rouge1_p, rouge1_r, rouge2_f, rougeL_f, rougeL_r]\n",
    "        except Exception:\n",
    "            return [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "    \n",
    "    df = df.with_columns(\n",
    "        pl.struct([f'{col}_1', f'{col}_2'])\n",
    "        .map_elements(rouge_scores, return_dtype=pl.List(pl.Float64))\n",
    "        .alias(f'{col}_rouge_metrics')\n",
    "    )\n",
    "    \n",
    "    df = df.with_columns([\n",
    "        pl.col(f'{col}_rouge_metrics').list.get(0).alias(f'{col}_rouge1_f'),\n",
    "        pl.col(f'{col}_rouge_metrics').list.get(1).alias(f'{col}_rouge1_precision'),\n",
    "        pl.col(f'{col}_rouge_metrics').list.get(2).alias(f'{col}_rouge1_recall'),\n",
    "        pl.col(f'{col}_rouge_metrics').list.get(3).alias(f'{col}_rouge2_f'),\n",
    "        pl.col(f'{col}_rouge_metrics').list.get(4).alias(f'{col}_rougeL_f'),\n",
    "        pl.col(f'{col}_rouge_metrics').list.get(5).alias(f'{col}_rougeL_recall')\n",
    "    ]).drop(f'{col}_rouge_metrics')\n",
    "    \n",
    "    pbar.close()\n",
    "    return df\n",
    "\n",
    "# for col in ('name', 'description',):\n",
    "#     df = calculate_rouge_metrics(df, col)\n",
    "#     gc.collect()\n",
    "\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e25a62cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df.filter(df['is_double'] != -1)\n",
    "test_df = df.filter(df['is_double'] == -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "21c379ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# удалим ненужные столбцы\n",
    "\n",
    "to_drop = [\n",
    "    'name_1', 'name_2', \n",
    "    'description_1', 'description_2', \n",
    "    'name_norm_1', 'name_norm_2',\n",
    "    'description_norm_1', 'description_norm_2', \n",
    "    'attr_vals_1', 'attr_vals_2',\n",
    "    'attr_keys_1', 'attr_keys_2',\n",
    "    'characteristic_attributes_mapping_1', 'characteristic_attributes_mapping_2',\n",
    "    'description_tokens_1', 'description_tokens_2',\n",
    "    'name_tokens_1', 'name_tokens_2', \n",
    "    'description_tokens_w_digits_1', 'description_tokens_w_digits_2',\n",
    "    'price_1', 'price_2',\n",
    "    'n_images_1', 'n_images_2',\n",
    "    'name_en_1', 'name_en_2',\n",
    "    'name_en_norm_1', 'name_en_norm_2',\n",
    "    'name_en_tokens_1', 'name_en_tokens_2',\n",
    "    'name_mix_1', 'name_mix_2',\n",
    "    'name_mix_norm_1', 'name_mix_norm_2',\n",
    "    'name_mix_tokens_1', 'name_mix_tokens_2',\n",
    "    'description_en_1', 'description_en_2',\n",
    "    'description_en_norm_1', 'description_en_norm_2',\n",
    "    'description_en_tokens_1', 'description_en_tokens_2',\n",
    "    'description_mix_1', 'description_mix_2',\n",
    "    'description_mix_norm_1', 'description_mix_norm_2',\n",
    "    'description_mix_tokens_1', 'description_mix_tokens_2',\n",
    "    'name_tokens_len_1', 'name_tokens_len_2',\n",
    "    'description_tokens_len_1', 'description_tokens_len_2',\n",
    "    'name_en_tokens_len_1', 'name_en_tokens_len_2',\n",
    "    'description_en_tokens_len_1', 'description_en_tokens_len_2',\n",
    "    'name_mix_tokens_len_1', 'name_mix_tokens_len_2',\n",
    "    'description_mix_tokens_len_1', 'description_mix_tokens_len_2',\n",
    "    'attr_keys_len_1', 'attr_keys_len_2',\n",
    "    'attr_vals_len_1', 'attr_vals_len_2',\n",
    "    'units_name_1', 'units_name_2',\n",
    "    'units_desc_1', 'units_desc_2',\n",
    "    'brands_name_1', 'brands_name_2',\n",
    "    'brands_desc_1', 'brands_desc_2',\n",
    "    'colors_name_1', 'colors_name_2',\n",
    "    'colors_desc_1', 'colors_desc_2',\n",
    "    'name_tokens_w_digits_1', 'name_tokens_w_digits_2',\n",
    "    'concat_keyval_1', 'concat_keyval_2'\n",
    "]\n",
    "\n",
    "train_df = train_df.drop(to_drop)\n",
    "test_df = test_df.drop(to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f0c79deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.write_parquet('../data/merged-with-features/train_df.parquet')\n",
    "test_df.write_parquet('../data/merged-with-features/test_df.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
