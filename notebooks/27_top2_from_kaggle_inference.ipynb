{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989212df-4ff1-4dc8-b97e-990a20723c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import timm\n",
    "\n",
    "from itertools import zip_longest\n",
    "import json\n",
    "import math\n",
    "import gc\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import Resize, RandomHorizontalFlip, ColorJitter, Normalize, Compose, RandomResizedCrop, \\\n",
    "    CenterCrop, ToTensor\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import joblib\n",
    "from scipy.sparse import hstack, vstack, csc_matrix, csr_matrix\n",
    "\n",
    "import networkx as nx\n",
    "from transformers import BertConfig, BertModel, BertTokenizerFast\n",
    "\n",
    "NUM_CLASSES = 11014\n",
    "NUM_WORKERS = 2\n",
    "SEED = 0\n",
    "device = 'cuda'\n",
    "\n",
    "def gem(x, p=3, eps=1e-6):\n",
    "    return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1. / p)\n",
    "\n",
    "\n",
    "class ShopeeNet(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 backbone,\n",
    "                 num_classes,\n",
    "                 fc_dim=512,\n",
    "                 s=30, margin=0.5, p=3):\n",
    "        super(ShopeeNet, self).__init__()\n",
    "\n",
    "        self.backbone = backbone\n",
    "        self.backbone.reset_classifier(num_classes=0)  # remove classifier\n",
    "\n",
    "        self.fc = nn.Linear(self.backbone.num_features, fc_dim)\n",
    "        self.bn = nn.BatchNorm1d(fc_dim)\n",
    "        self._init_params()\n",
    "        self.p = p\n",
    "\n",
    "    def _init_params(self):\n",
    "        nn.init.xavier_normal_(self.fc.weight)\n",
    "        nn.init.constant_(self.fc.bias, 0)\n",
    "        nn.init.constant_(self.bn.weight, 1)\n",
    "        nn.init.constant_(self.bn.bias, 0)\n",
    "\n",
    "    def extract_feat(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.backbone.forward_features(x)\n",
    "        if isinstance(x, tuple):\n",
    "            x = (x[0] + x[1]) / 2\n",
    "            x = self.bn(x)\n",
    "        else:\n",
    "            x = gem(x, p=self.p).view(batch_size, -1)\n",
    "            x = self.fc(x)\n",
    "            x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, label):\n",
    "        feat = self.extract_feat(x)\n",
    "        x = self.loss_module(feat, label)\n",
    "        return x, feat\n",
    "\n",
    "\n",
    "def batch_images_generator(path_to_folder, bs, preprocessor, resume_from_batch=None):\n",
    "    import glob\n",
    "    from torchvision.io import read_image\n",
    "\n",
    "    images = glob.glob('*.jpg', root_dir=path_to_folder)\n",
    "\n",
    "    images = [image for image in images if os.path.getsize(path_to_folder + image) > 0]\n",
    "    print(len(images))\n",
    "    start = 0 if not resume_from_batch else resume_from_batch\n",
    "    for i in tqdm(range(start, len(images), bs)):\n",
    "        batch_images = images[i:i + bs]\n",
    "        torch_batch = []\n",
    "        for x in batch_images:\n",
    "            try:\n",
    "                torch_batch.append(transform(read_image(path_to_folder + x, mode='RGB').to('cuda').float() / 255)[None])\n",
    "            except Exception as e:\n",
    "                print(read_image(path_to_folder + x).shape)\n",
    "                print(e)\n",
    "                print(x)\n",
    "\n",
    "        torch_batch = torch.cat(torch_batch).to(device)\n",
    "\n",
    "        yield i, torch_batch, batch_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436d0384-cc67-49ec-add4-ddc0be6baeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint1 = torch.load('weights/top2/v45.pth')\n",
    "checkpoint2 = torch.load('weights/top2/v34.pth')\n",
    "\n",
    "params1 = checkpoint1['params']\n",
    "params2 = checkpoint2['params']\n",
    "params1['backbone'] = 'deit_base_distilled_patch16_384.fb_in1k'\n",
    "\n",
    "transform = Compose([\n",
    "    Resize(size=params1['test_size'] + 32, interpolation=Image.BICUBIC),\n",
    "    CenterCrop((params1['test_size'], params1['test_size'])),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# backbone = timm.create_model(model_name=params1['backbone'], pretrained=False)\n",
    "# model1 = ShopeeNet(backbone, num_classes=0, fc_dim=params1['fc_dim'])\n",
    "# model1 = model1.to('cuda')\n",
    "# model1.load_state_dict(checkpoint1['model'], strict=False)\n",
    "# model1.train(False)\n",
    "# model1.p = params1['p_eval']\n",
    "\n",
    "backbone = timm.create_model(model_name=params2['backbone'], pretrained=False)\n",
    "model2 = ShopeeNet(backbone, num_classes=0, fc_dim=params2['fc_dim'])\n",
    "model2 = model2.to('cuda')\n",
    "model2.load_state_dict(checkpoint2['model'], strict=False)\n",
    "model2.train(False)\n",
    "model2.p = params2['p_eval']\n",
    "\n",
    "img_feats1 = []\n",
    "img_feats2 = []\n",
    "\n",
    "img_hs = []\n",
    "img_ws = []\n",
    "st_sizes = []\n",
    "\n",
    "batch_size = 64\n",
    "for t in ['train', 'test']:\n",
    "    output_dir = f'avito/images/{t}/parquets'\n",
    "    # os.mkdir(output_dir, exist)\n",
    "    # os.mkdir(output_dir, exist_ok=True)\n",
    "    images_dir = f'avito/images/{t}/images/'\n",
    "\n",
    "    for i, batch, fname in batch_images_generator(images_dir, bs=batch_size, preprocessor=None):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # feats_minibatch1 = model1.extract_feat(batch)\n",
    "            # img_feats1.append(feats_minibatch1.cpu().numpy())\n",
    "            feats_minibatch2 = model2.extract_feat(batch).tolist()\n",
    "\n",
    "        df = pd.DataFrame({\n",
    "            \"filename\": fname,\n",
    "            \"embedding\": feats_minibatch2  # list of np.array, stored as arrays\n",
    "        })\n",
    "\n",
    "        df.to_parquet(\n",
    "            os.path.join(output_dir, f\"batch_{i // batch_size}.parquet\"),\n",
    "            index=False\n",
    "        )\n",
    "        \n",
    "    import duckdb\n",
    "    duckdb.sql(rf\"\"\"\n",
    "        COPY (\n",
    "            SELECT * FROM '{output_dir}/*.parquet'\n",
    "        )\n",
    "        TO '{output_dir}/final_embeddings.parquet' (FORMAT PARQUET)\n",
    "    \"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
