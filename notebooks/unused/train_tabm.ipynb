{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe8b78d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T12:09:13.557667Z",
     "iopub.status.busy": "2025-05-23T12:09:13.557386Z",
     "iopub.status.idle": "2025-05-23T12:10:32.518441Z",
     "shell.execute_reply": "2025-05-23T12:10:32.517150Z",
     "shell.execute_reply.started": "2025-05-23T12:09:13.557630Z"
    },
    "papermill": {
     "duration": 39.405546,
     "end_time": "2025-02-01T09:24:25.367679",
     "exception": false,
     "start_time": "2025-02-01T09:23:45.962133",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for autograd-gamma (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement scikit-learn==1.4.0 (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for scikit-learn==1.4.0\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install lifelines -q --no-index --find-links=cibmtr2024-import/lifelines\n",
    "!pip install scikit-learn==1.4.0 -q --no-index --find-links=cibmtr2024-import/scikit_learn\n",
    "!pip install rtdl_num_embeddings -q --no-index --find-links=cibmtr2024-import/rtdl_num_embeddings\n",
    "!pip install delu -q --no-index --find-links=cibmtr2024-import/delu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b75fba56",
   "metadata": {
    "_cell_guid": "f9153f3d-b8cd-4900-8ad0-41c0c533817a",
    "_uuid": "02935884-0633-4f67-8ded-1fed6a1abea9",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-05-23T12:10:32.520285Z",
     "iopub.status.busy": "2025-05-23T12:10:32.519943Z",
     "iopub.status.idle": "2025-05-23T12:10:38.256073Z",
     "shell.execute_reply": "2025-05-23T12:10:38.254904Z",
     "shell.execute_reply.started": "2025-05-23T12:10:32.520256Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 4.888014,
     "end_time": "2025-02-01T09:24:30.259979",
     "exception": false,
     "start_time": "2025-02-01T09:24:25.371965",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tabm_reference import Model, make_parameter_groups\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "import rtdl_num_embeddings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedGroupKFold\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "import os\n",
    "import gc\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import joblib\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset, ConcatDataset\n",
    "\n",
    "import delu\n",
    "import math\n",
    "\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfa3fc2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T12:10:38.258964Z",
     "iopub.status.busy": "2025-05-23T12:10:38.258471Z",
     "iopub.status.idle": "2025-05-23T12:10:38.265358Z",
     "shell.execute_reply": "2025-05-23T12:10:38.264295Z",
     "shell.execute_reply.started": "2025-05-23T12:10:38.258937Z"
    },
    "papermill": {
     "duration": 0.321439,
     "end_time": "2025-02-01T09:24:30.585552",
     "exception": false,
     "start_time": "2025-02-01T09:24:30.264113",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "MAIN_PATH = 'avito-train-combined/data/avito-merged-dataset/'\n",
    "ECOM_PRETRAIN = 'avito-train-combined/data/clip-marqofashionsiglip-marqoecom-top2kaggle/'\n",
    "RESNET_PATH = 'avito-train-combined/data/resnet-cossim/'\n",
    "KAGGLE_TOP5 = 'avito-train-combined/data/top5-kaggle/'\n",
    "BERTA_PATH = 'avito-train-combined/data/berta-pretrained-cossims/'\n",
    "RUBERT_TINY_OOF_PATH = 'avito-train-combined/data/rubert-folds/'\n",
    "RUBERT_TINY_PREDS_PATH = 'avito-train-combined/data/rubert-test-preds/'\n",
    "E5LARGE_OOF_PATH = 'avito-train-combined/data/avito-e5-large-pretrain/'\n",
    "E5LARGE_PREDS_PATH = 'avito-train-combined/data/avito-e5-large-test/'\n",
    "REV_RUBERT_TINY_OOF_PATH = 'avito-train-combined/data/name_desc_bert_oof_rev/'\n",
    "REV_RUBERT_TINY_PREDS_PATH = 'avito-train-combined/data/name-desc-bert-preds-rev/'\n",
    "USERBGE_COSSIMS_PATH = 'avito-train-combined/data/userbge-cossims/'\n",
    "RUBERT_BASE_TEST_PREDS_PATH = 'avito-train-combined/data/rubert-fixed-test-preds/'\n",
    "RUBERT_BASE_TEST_PREDS_REV_PATH = 'avito-train-combined/data/rubert-fixed-test-preds-rev/'\n",
    "FT_PREDS_PATH = 'avito-train-combined/data/ft-preds2/'\n",
    "RUBERT_BASE_OOF_PATH = 'avito-train-combined/data/trained-rubert-base-preds/'\n",
    "RUBERT_BASE_OOF_REV_PATH = 'avito-train-combined/data/trained-rubert-base-preds-rev/'\n",
    "ROUGE_PATH = 'rouge-avito/'\n",
    "\n",
    "USE_MEAN_BASE_AND_REV = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a705a52-e3ee-495f-a865-b2c98d33ba92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T12:10:38.266712Z",
     "iopub.status.busy": "2025-05-23T12:10:38.266333Z",
     "iopub.status.idle": "2025-05-23T12:11:02.909098Z",
     "shell.execute_reply": "2025-05-23T12:11:02.908117Z",
     "shell.execute_reply.started": "2025-05-23T12:10:38.266641Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_parquet(MAIN_PATH + 'train_df.parquet')\n",
    "test = pd.read_parquet(MAIN_PATH + 'test_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c880d45c-1875-47f6-8475-36db9f11a41c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T12:11:02.910952Z",
     "iopub.status.busy": "2025-05-23T12:11:02.910539Z",
     "iopub.status.idle": "2025-05-23T12:11:22.088059Z",
     "shell.execute_reply": "2025-05-23T12:11:22.086981Z",
     "shell.execute_reply.started": "2025-05-23T12:11:02.910922Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_drop = [\n",
    "    'category_level_1_1', 'category_level_1_2',\n",
    "    'category_level_2_1', 'category_level_2_2',\n",
    "    'category_level_3_1', 'category_level_3_2',\n",
    "    'category_level_4_1', 'category_level_4_2',\n",
    "]\n",
    "\n",
    "train[to_drop] = train[to_drop].fillna('none')\n",
    "\n",
    "train['unique_cat_1'] = train['category_level_1_1'] + '_' + train['category_level_1_2']\n",
    "train['unique_cat_2'] = train['category_level_2_1'] + '_' + train['category_level_2_2']\n",
    "train['unique_cat_3'] = train['category_level_3_1'] + '_' + train['category_level_3_2']\n",
    "train['unique_cat_4'] = train['category_level_4_1'] + '_' + train['category_level_4_2']\n",
    "\n",
    "test[to_drop] = test[to_drop].fillna('none')\n",
    "\n",
    "test['unique_cat_1'] = test['category_level_1_1'] + '_' + test['category_level_1_2']\n",
    "test['unique_cat_2'] = test['category_level_2_1'] + '_' + test['category_level_2_2']\n",
    "test['unique_cat_3'] = test['category_level_3_1'] + '_' + test['category_level_3_2']\n",
    "test['unique_cat_4'] = test['category_level_4_1'] + '_' + test['category_level_4_2']\n",
    "\n",
    "train.drop(columns=to_drop, axis=1, inplace=True)\n",
    "gc.collect()\n",
    "\n",
    "test.drop(columns=to_drop, axis=1, inplace=True)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "123dd232-2936-4622-9fdf-725d2470d399",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T12:11:22.089382Z",
     "iopub.status.busy": "2025-05-23T12:11:22.089101Z",
     "iopub.status.idle": "2025-05-23T12:11:52.785915Z",
     "shell.execute_reply": "2025-05-23T12:11:52.784731Z",
     "shell.execute_reply.started": "2025-05-23T12:11:22.089353Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Колонки с inf: ['name_tanimoto', 'name_norm_tanimoto', 'name_en_tanimoto', 'name_mix_tanimoto', 'description_en_tanimoto', 'description_mix_tanimoto', 'name_tokens_w_digits_tanimoto', 'description_tokens_w_digits_tanimoto']\n"
     ]
    }
   ],
   "source": [
    "numerical_features = train.select_dtypes(\n",
    "    include=['float16', 'float32', 'float64', 'int8', 'int16', 'int32', 'int64']\n",
    ").columns.to_list()\n",
    "\n",
    "cols_with_inf = train[numerical_features].columns[np.isinf(train[numerical_features]).any()].tolist()\n",
    "\n",
    "print(\"Колонки с inf:\", cols_with_inf)\n",
    "\n",
    "train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "test.replace([np.inf, -np.inf], np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd16218e-fd30-4dfa-a7ce-998f2ab2af93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T12:11:52.787614Z",
     "iopub.status.busy": "2025-05-23T12:11:52.787288Z",
     "iopub.status.idle": "2025-05-23T12:11:52.800135Z",
     "shell.execute_reply": "2025-05-23T12:11:52.798984Z",
     "shell.execute_reply.started": "2025-05-23T12:11:52.787586Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                    gc.collect()\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                    gc.collect()\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                    gc.collect()\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "                    gc.collect()\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                    gc.collect()\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                    gc.collect()\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "                    gc.collect()\n",
    "        else:\n",
    "            if df[col].nunique() == 2:\n",
    "                df[col] = df[col].astype('bool')\n",
    "            gc.collect()\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "\n",
    "    gc.collect()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4aeae0ce-7cfd-4b9c-b449-4110e0da37ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T12:11:52.801767Z",
     "iopub.status.busy": "2025-05-23T12:11:52.801391Z",
     "iopub.status.idle": "2025-05-23T12:13:16.107721Z",
     "shell.execute_reply": "2025-05-23T12:13:16.106715Z",
     "shell.execute_reply.started": "2025-05-23T12:11:52.801725Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 6521.05 MB\n",
      "Memory usage after optimization is: 1756.63 MB\n",
      "Decreased by 73.1%\n"
     ]
    }
   ],
   "source": [
    "train = reduce_mem_usage(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e59096b2-ad3a-49c7-be1e-8be81aad3e8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T12:13:16.111763Z",
     "iopub.status.busy": "2025-05-23T12:13:16.111423Z",
     "iopub.status.idle": "2025-05-23T12:14:28.482417Z",
     "shell.execute_reply": "2025-05-23T12:14:28.481514Z",
     "shell.execute_reply.started": "2025-05-23T12:13:16.111739Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 1728.06 MB\n",
      "Memory usage after optimization is: 467.78 MB\n",
      "Decreased by 72.9%\n"
     ]
    }
   ],
   "source": [
    "test = reduce_mem_usage(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6788a60a-ac6a-4932-98b2-e134cc686e79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T12:14:28.483760Z",
     "iopub.status.busy": "2025-05-23T12:14:28.483409Z",
     "iopub.status.idle": "2025-05-23T12:14:49.728000Z",
     "shell.execute_reply": "2025-05-23T12:14:49.726801Z",
     "shell.execute_reply.started": "2025-05-23T12:14:28.483727Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train = train.sort_values(by=['variantid_1', 'variantid_2'])\n",
    "test = test.sort_values(by=['variantid_1', 'variantid_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba261a83-e63a-4fb0-88b5-8217efdc8699",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T12:14:49.730053Z",
     "iopub.status.busy": "2025-05-23T12:14:49.729671Z",
     "iopub.status.idle": "2025-05-23T12:19:55.721286Z",
     "shell.execute_reply": "2025-05-23T12:19:55.720145Z",
     "shell.execute_reply.started": "2025-05-23T12:14:49.730021Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "userbge_cossims_train.shape=(125303, 3)\n",
      "userbge_cossims_train.shape=(250606, 3)\n",
      "userbge_cossims_train.shape=(375909, 3)\n",
      "userbge_cossims_train.shape=(501212, 3)\n",
      "userbge_cossims_train.shape=(626515, 3)\n",
      "userbge_cossims_train.shape=(751818, 3)\n",
      "userbge_cossims_train.shape=(877121, 3)\n",
      "userbge_cossims_train.shape=(1002424, 3)\n",
      "userbge_cossims_train.shape=(1127727, 3)\n",
      "userbge_cossims_train.shape=(1253030, 3)\n",
      "userbge_cossims_train.shape=(1378333, 3)\n",
      "userbge_cossims_train.shape=(1503636, 3)\n",
      "userbge_cossims_train.shape=(1628939, 3)\n",
      "userbge_cossims_train.shape=(1754242, 3)\n",
      "userbge_cossims_train.shape=(1879555, 3)\n",
      "userbge_cossims_test.shape=(125000, 3)\n",
      "userbge_cossims_test.shape=(250000, 3)\n",
      "userbge_cossims_test.shape=(375000, 3)\n",
      "userbge_cossims_test.shape=(500000, 3)\n"
     ]
    }
   ],
   "source": [
    "# --- IMG FEATURES ---\n",
    "\n",
    "# pretrain clip\n",
    "train_clip = pd.read_parquet(ECOM_PRETRAIN + 'cossim_final_embeddings_train_CLIP.parquet')\n",
    "test_clip = pd.read_parquet(ECOM_PRETRAIN + 'cossim_final_embeddings_test_CLIP.parquet')\n",
    "\n",
    "train_clip = train_clip.sort_values(by=['variantid_1', 'variantid_2'])\n",
    "test_clip = test_clip.sort_values(by=['variantid_1', 'variantid_2'])\n",
    "train['clip_cosine_sim'] = train_clip['cosine_sim']\n",
    "test['clip_cosine_sim'] = test_clip['cosine_sim']\n",
    "del train_clip, test_clip\n",
    "\n",
    "# pretrain fashion siglip\n",
    "train_fashionsiglip = pd.read_parquet(ECOM_PRETRAIN + 'cossim_final_embeddings_fashion_clip_train.parquet')\n",
    "test_fashionsiglip = pd.read_parquet(ECOM_PRETRAIN + 'cossim_final_embeddings_fashion_clip_test.parquet')\n",
    "\n",
    "train_fashionsiglip = train_fashionsiglip.sort_values(by=['variantid_1', 'variantid_2'])\n",
    "test_fashionsiglip = test_fashionsiglip.sort_values(by=['variantid_1', 'variantid_2'])\n",
    "train['fashionsiglip_cosine_sim'] = train_fashionsiglip['cosine_sim']\n",
    "test['fashionsiglip_cosine_sim'] = test_fashionsiglip['cosine_sim']\n",
    "del train_fashionsiglip, test_fashionsiglip\n",
    "\n",
    "# pretrain marqo ecom\n",
    "train_ecom = pd.read_parquet(ECOM_PRETRAIN + 'cossim_final_embeddings_ecomm_train.parquet')\n",
    "test_ecom = pd.read_parquet(ECOM_PRETRAIN + 'cossim_final_embeddings_ecomm_test.parquet')\n",
    "\n",
    "train_ecom = train_ecom.sort_values(by=['variantid_1', 'variantid_2'])\n",
    "test_ecom = test_ecom.sort_values(by=['variantid_1', 'variantid_2'])\n",
    "train['ecom_cosine_sim'] = train_ecom['cosine_sim']\n",
    "test['ecom_cosine_sim'] = test_ecom['cosine_sim']\n",
    "del train_ecom, test_ecom\n",
    "\n",
    "# kaggle top2 model, bugged\n",
    "# train_top2kaggle = pd.read_parquet(ECOM_PRETRAIN + 'cossim_final_top2_kaggle_train.parquet')\n",
    "# test_top2kaggle = pd.read_parquet(ECOM_PRETRAIN + 'cossim_final_top2_kaggle_test.parquet')\n",
    "\n",
    "# train_top2kaggle = train_top2kaggle.sort_values(by=['variantid_1', 'variantid_2'])\n",
    "# test_top2kaggle = test_top2kaggle.sort_values(by=['variantid_1', 'variantid_2'])\n",
    "# train['top2kaggle_cosine_sim'] = train_top2kaggle['cosine_sim']\n",
    "# test['top2kaggle_cosine_sim'] = test_top2kaggle['cosine_sim']\n",
    "# del train_top2kaggle, test_top2kaggle\n",
    "\n",
    "# kaggle top5 model\n",
    "train_top5kaggle = pd.read_parquet(KAGGLE_TOP5 + 'cossim_final_concat_train.parquet')\n",
    "test_top5kaggle = pd.read_parquet(KAGGLE_TOP5 + 'cossim_final_concat_test.parquet')\n",
    "\n",
    "train_top5kaggle = train_top5kaggle.sort_values(by=['variantid_1', 'variantid_2'])\n",
    "test_top5kaggle = test_top5kaggle.sort_values(by=['variantid_1', 'variantid_2'])\n",
    "train['top5kaggle_cosine_sim'] = train_top5kaggle['cosine_sim']\n",
    "test['top5kaggle_cosine_sim'] = test_top5kaggle['cosine_sim']\n",
    "del train_top5kaggle, test_top5kaggle\n",
    "\n",
    "# trained resnet\n",
    "train_resnet = pd.read_parquet(RESNET_PATH + 'train_resnet_cossim.parquet')\n",
    "test_resnet = pd.read_parquet(RESNET_PATH + 'test_resnet_cossim.parquet')\n",
    "\n",
    "train_resnet = train_resnet.sort_values(by=['variantid_1', 'variantid_2']).reset_index(drop=True)\n",
    "test_resnet = test_resnet.sort_values(by=['variantid_1', 'variantid_2']).reset_index(drop=True)\n",
    "train['resnet_cosine_sim'] = train_resnet['cossims_resnet']\n",
    "test['resnet_cosine_sim'] = test_resnet['cossims_resnet']\n",
    "del train_resnet, test_resnet\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "# --- TEXT FEATURES ---\n",
    "\n",
    "# pretrain berta\n",
    "berta_cossims_train_part1 = pd.read_parquet(BERTA_PATH + 'berta_cossims_train_part1.parquet')\n",
    "berta_cossims_train_part2 = pd.read_parquet(BERTA_PATH + 'berta_cossims_train_part2.parquet')\n",
    "berta_cossims_train = pd.concat([berta_cossims_train_part1, berta_cossims_train_part2])\n",
    "berta_cossims_test = pd.read_parquet(BERTA_PATH + 'berta_cossims_test.parquet')\n",
    "\n",
    "berta_cossims_train = berta_cossims_train.sort_values(by=['variantid_1', 'variantid_2'])\n",
    "berta_cossims_test = berta_cossims_test.sort_values(by=['variantid_1', 'variantid_2'])\n",
    "train['berta_cossim'] = berta_cossims_train['berta_cossim']\n",
    "test['berta_cossim'] = berta_cossims_test['berta_cossim']\n",
    "\n",
    "del berta_cossims_train_part1, berta_cossims_train_part2, berta_cossims_train, berta_cossims_test\n",
    "gc.collect()\n",
    "\n",
    "# trained rubert (cherez zhopu rukamu obuchen)\n",
    "rubert_oof_fold0 = pd.read_parquet(RUBERT_TINY_OOF_PATH + 'name_desc_bert_fold0.parquet')\n",
    "rubert_oof_fold1 = pd.read_parquet(RUBERT_TINY_OOF_PATH + 'name_desc_bert_fold1.parquet')\n",
    "rubert_oof_fold2 = pd.read_parquet(RUBERT_TINY_OOF_PATH + 'name_desc_bert_fold2.parquet')\n",
    "rubert_oof_fold3 = pd.read_parquet(RUBERT_TINY_OOF_PATH + 'name_desc_bert_fold3.parquet')\n",
    "rubert_oof_fold4 = pd.read_parquet(RUBERT_TINY_OOF_PATH + 'name_desc_bert_fold4.parquet')\n",
    "\n",
    "rubert_test_pred_fold0 = pd.read_parquet(RUBERT_TINY_PREDS_PATH + 'name_desc_rubert_tiny_turbo_2048_wce_0.parquet')\n",
    "rubert_test_pred_fold1 = pd.read_parquet(RUBERT_TINY_PREDS_PATH + 'name_desc_rubert_tiny_turbo_2048_wce_1.parquet')\n",
    "rubert_test_pred_fold2 = pd.read_parquet(RUBERT_TINY_PREDS_PATH + 'name_desc_rubert_tiny_turbo_2048_wce_2.parquet')\n",
    "rubert_test_pred_fold3 = pd.read_parquet(RUBERT_TINY_PREDS_PATH + 'name_desc_rubert_tiny_turbo_2048_wce_3.parquet')\n",
    "rubert_test_pred_fold4 = pd.read_parquet(RUBERT_TINY_PREDS_PATH + 'name_desc_rubert_tiny_turbo_2048_wce_4.parquet')\n",
    "\n",
    "rubert_oof_fold0.rename(columns={'name_desc_bert_oof1': 'name_desc_rubert_tiny_turbo_2048_wce'}, inplace=True)\n",
    "rubert_oof_fold1.rename(columns={'name_desc_bert_oof4': 'name_desc_rubert_tiny_turbo_2048_wce'}, inplace=True)\n",
    "rubert_oof_fold2.rename(columns={'name_desc_bert_oof4': 'name_desc_rubert_tiny_turbo_2048_wce'}, inplace=True)\n",
    "rubert_oof_fold3.rename(columns={'name_desc_bert_oof4': 'name_desc_rubert_tiny_turbo_2048_wce'}, inplace=True)\n",
    "rubert_oof_fold4.rename(columns={'name_desc_bert_oof4': 'name_desc_rubert_tiny_turbo_2048_wce'}, inplace=True)\n",
    "\n",
    "rubert_oof_fold0 = rubert_oof_fold0.sort_values(by=['variantid_1', 'variantid_2'])\n",
    "rubert_oof_fold1 = rubert_oof_fold1.sort_values(by=['variantid_1', 'variantid_2'])\n",
    "rubert_oof_fold2 = rubert_oof_fold2.sort_values(by=['variantid_1', 'variantid_2'])\n",
    "rubert_oof_fold3 = rubert_oof_fold3.sort_values(by=['variantid_1', 'variantid_2'])\n",
    "rubert_oof_fold4 = rubert_oof_fold4.sort_values(by=['variantid_1', 'variantid_2'])\n",
    "\n",
    "rubert_oof = rubert_oof_fold0['name_desc_rubert_tiny_turbo_2048_wce'] + rubert_oof_fold1['name_desc_rubert_tiny_turbo_2048_wce'] + rubert_oof_fold2['name_desc_rubert_tiny_turbo_2048_wce'] + rubert_oof_fold3['name_desc_rubert_tiny_turbo_2048_wce'] + rubert_oof_fold4['name_desc_rubert_tiny_turbo_2048_wce']\n",
    "\n",
    "rubert_test_pred_fold0 = rubert_test_pred_fold0.sort_values(by=['variantid_1', 'variantid_2'])\n",
    "rubert_test_pred_fold1 = rubert_test_pred_fold1.sort_values(by=['variantid_1', 'variantid_2'])\n",
    "rubert_test_pred_fold2 = rubert_test_pred_fold2.sort_values(by=['variantid_1', 'variantid_2'])\n",
    "rubert_test_pred_fold3 = rubert_test_pred_fold3.sort_values(by=['variantid_1', 'variantid_2'])\n",
    "rubert_test_pred_fold4 = rubert_test_pred_fold4.sort_values(by=['variantid_1', 'variantid_2'])\n",
    "\n",
    "rubert_preds = (rubert_test_pred_fold0['name_desc_rubert_tiny_turbo_2048_wce_0'] + rubert_test_pred_fold1['name_desc_rubert_tiny_turbo_2048_wce_1'] + rubert_test_pred_fold2['name_desc_rubert_tiny_turbo_2048_wce_2'] + rubert_test_pred_fold3['name_desc_rubert_tiny_turbo_2048_wce_3'] + rubert_test_pred_fold4['name_desc_rubert_tiny_turbo_2048_wce_4']) / 5\n",
    "\n",
    "train['name_desc_rubert_tiny_turbo_2048_wce'] = rubert_oof\n",
    "test['name_desc_rubert_tiny_turbo_2048_wce'] = rubert_preds\n",
    "\n",
    "del rubert_oof_fold0, rubert_oof_fold1, rubert_oof_fold2, rubert_oof_fold3, rubert_oof_fold4\n",
    "del rubert_oof\n",
    "del rubert_test_pred_fold0, rubert_test_pred_fold1, rubert_test_pred_fold2, rubert_test_pred_fold3, rubert_test_pred_fold4\n",
    "del rubert_preds\n",
    "gc.collect()\n",
    "\n",
    "# pretrain e5large\n",
    "e5large_cossims_train_part1 = pd.read_parquet(E5LARGE_OOF_PATH + 'e5large_cossims_fold0.parquet')\n",
    "e5large_cossims_train_part2 = pd.read_parquet(E5LARGE_OOF_PATH + 'e5large_cossims_fold1.parquet')\n",
    "e5large_cossims_train_part3 = pd.read_parquet(E5LARGE_OOF_PATH + 'e5large_cossims_fold2.parquet')\n",
    "e5large_cossims_train_part4 = pd.read_parquet(E5LARGE_OOF_PATH + 'e5large_cossims_fold3.parquet')\n",
    "e5large_cossims_train_part5 = pd.read_parquet(E5LARGE_OOF_PATH + 'e5large_cossims_fold4.parquet')\n",
    "\n",
    "e5large_cossims_train = pd.concat([\n",
    "    e5large_cossims_train_part1,\n",
    "    e5large_cossims_train_part2,\n",
    "    e5large_cossims_train_part3,\n",
    "    e5large_cossims_train_part4,\n",
    "    e5large_cossims_train_part5\n",
    "])\n",
    "e5large_cossims_train = e5large_cossims_train.sort_values(by=['variantid_1', 'variantid_2'])\n",
    "\n",
    "del e5large_cossims_train_part1, e5large_cossims_train_part2, e5large_cossims_train_part3, e5large_cossims_train_part4, e5large_cossims_train_part5\n",
    "gc.collect()\n",
    "\n",
    "e5large_cossims_test_part1 = pd.read_parquet(E5LARGE_PREDS_PATH + 'e5large_cossims_part1.parquet')\n",
    "e5large_cossims_test_part2 = pd.read_parquet(E5LARGE_PREDS_PATH + 'e5large_cossims_part2.parquet')\n",
    "\n",
    "e5large_cossims_test = pd.concat([\n",
    "    e5large_cossims_test_part1,\n",
    "    e5large_cossims_test_part2,\n",
    "])\n",
    "e5large_cossims_test = e5large_cossims_test.sort_values(by=['variantid_1', 'variantid_2'])\n",
    "\n",
    "del e5large_cossims_test_part1, e5large_cossims_test_part2\n",
    "gc.collect()\n",
    "\n",
    "train['e5large_cossim'] = e5large_cossims_train['e5large_cossim']\n",
    "test['e5large_cossim'] = e5large_cossims_test['e5large_cossim']\n",
    "\n",
    "del e5large_cossims_train, e5large_cossims_test\n",
    "gc.collect()\n",
    "\n",
    "# rev trained rubert (cherez zhopu rukamu obuchen)\n",
    "rubert_oof_rev = pd.read_parquet(REV_RUBERT_TINY_OOF_PATH + 'name_desc_bert_oof_rev.parquet')\n",
    "rubert_preds_rev = pd.read_parquet(REV_RUBERT_TINY_PREDS_PATH + 'name_desc_bert_preds_rev.parquet')\n",
    "\n",
    "rubert_oof_rev = rubert_oof_rev.sort_values(by=['variantid_1', 'variantid_2'])\n",
    "rubert_preds_rev = rubert_preds_rev.sort_values(by=['variantid_1', 'variantid_2'])\n",
    "train['name_desc_rubert_tiny_turbo_2048_wce_rev'] = rubert_oof_rev['name_desc_bert_oof_rev']\n",
    "test['name_desc_rubert_tiny_turbo_2048_wce_rev'] = rubert_preds_rev['name_desc_bert_preds_rev']\n",
    "del rubert_oof_rev, rubert_preds_rev\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "# rubert tta\n",
    "if USE_MEAN_BASE_AND_REV:\n",
    "    train['name_desc_rubert_tiny_turbo_2048_wce_tta'] = (train['name_desc_rubert_tiny_turbo_2048_wce'] + train['name_desc_rubert_tiny_turbo_2048_wce_rev']) / 2\n",
    "    test['name_desc_rubert_tiny_turbo_2048_wce_tta'] = (test['name_desc_rubert_tiny_turbo_2048_wce'] + test['name_desc_rubert_tiny_turbo_2048_wce_rev']) / 2\n",
    "    del train['name_desc_rubert_tiny_turbo_2048_wce'], train['name_desc_rubert_tiny_turbo_2048_wce_rev']\n",
    "    del test['name_desc_rubert_tiny_turbo_2048_wce'], test['name_desc_rubert_tiny_turbo_2048_wce_rev']\n",
    "    gc.collect()\n",
    "\n",
    "# pretrain userbge\n",
    "userbge_cossims_train = pd.DataFrame()\n",
    "userbge_cossims_test = pd.DataFrame()\n",
    "\n",
    "for i in range(1, 16):\n",
    "    curr_df = pd.read_parquet(USERBGE_COSSIMS_PATH + f'userbge_cossims_train_part{i}.parquet')\n",
    "    userbge_cossims_train = pd.concat([userbge_cossims_train, curr_df])\n",
    "    print(f'{userbge_cossims_train.shape=}')\n",
    "userbge_cossims_train = userbge_cossims_train.sort_values(by=['variantid_1', 'variantid_2'])\n",
    "\n",
    "for i in range(1, 5):\n",
    "    curr_df = pd.read_parquet(USERBGE_COSSIMS_PATH + f'/userbge_cossims_test_part{i}.parquet')\n",
    "    userbge_cossims_test = pd.concat([userbge_cossims_test, curr_df])\n",
    "    print(f'{userbge_cossims_test.shape=}')\n",
    "userbge_cossims_test = userbge_cossims_test.sort_values(by=['variantid_1', 'variantid_2'])\n",
    "\n",
    "del curr_df\n",
    "gc.collect()\n",
    "\n",
    "train['userbge_cossim'] = userbge_cossims_train['userbge_cossim']\n",
    "test['userbge_cossim'] = userbge_cossims_test['userbge_cossim']\n",
    "\n",
    "del userbge_cossims_train, userbge_cossims_test\n",
    "gc.collect()\n",
    "\n",
    "# --- тут именно до сэмпла! ---\n",
    "\n",
    "# trained rubert-base test preds\n",
    "rubert_base_pred = pd.read_csv(\n",
    "    RUBERT_BASE_TEST_PREDS_PATH + 'rubert_ZAEBAL_SUKA.csv'\n",
    ").rename(columns={'base_id': 'variantid_1', 'cand_id': 'variantid_2'})\n",
    "rubert_base_pred = rubert_base_pred.sort_values(by=['variantid_1', 'variantid_2']).reset_index(drop=True)\n",
    "test['rubert_base_trained'] = rubert_base_pred['probability']\n",
    "\n",
    "rubert_base_pred_rev = pd.read_csv(\n",
    "    RUBERT_BASE_TEST_PREDS_REV_PATH + 'rubert_ZAEBAL_SUKA_REV.csv'\n",
    ").rename(columns={'base_id': 'variantid_1', 'cand_id': 'variantid_2'})\n",
    "rubert_base_pred_rev = rubert_base_pred_rev.sort_values(by=['variantid_1', 'variantid_2']).reset_index(drop=True)\n",
    "test['rubert_base_trained_rev'] = rubert_base_pred_rev['probability']\n",
    "\n",
    "del rubert_base_pred, rubert_base_pred_rev\n",
    "gc.collect()\n",
    "\n",
    "# rubert base test tta\n",
    "if USE_MEAN_BASE_AND_REV:\n",
    "    test['rubert_base_trained_tta'] = (test['rubert_base_trained'] + test['rubert_base_trained_rev']) / 2\n",
    "    del test['rubert_base_trained'],  test['rubert_base_trained_rev']\n",
    "    gc.collect()\n",
    "\n",
    "# --- sample!!! ---\n",
    "train = train.sample(len(train), random_state=42)\n",
    "\n",
    "# trained ft (именно после сэмпла! тут уже все карты в нужном порядке разложены)\n",
    "fasttext_train = joblib.load(FT_PREDS_PATH + 'oof_preds.pkl')\n",
    "fasttext_test = joblib.load(FT_PREDS_PATH + 'test_preds.pkl')\n",
    "fasttext_train_rev = joblib.load(FT_PREDS_PATH + 'oof_preds_rev.pkl')\n",
    "fasttext_test_rev = joblib.load(FT_PREDS_PATH + 'test_preds_rev.pkl')\n",
    "\n",
    "train['fasttext'] = fasttext_train\n",
    "test['fasttext'] = fasttext_test\n",
    "train['fasttext_rev'] = fasttext_train_rev\n",
    "test['fasttext_rev'] = fasttext_test_rev\n",
    "\n",
    "del fasttext_train, fasttext_test, fasttext_train_rev, fasttext_test_rev\n",
    "gc.collect()\n",
    "\n",
    "# fasttext tta\n",
    "if USE_MEAN_BASE_AND_REV:\n",
    "    train['fasttext_tta'] = (train['fasttext'] + train['fasttext_rev']) / 2\n",
    "    test['fasttext_tta'] = (test['fasttext'] + test['fasttext_rev']) / 2\n",
    "    del train['fasttext'], train['fasttext_rev']\n",
    "    del test['fasttext'], test['fasttext_rev']\n",
    "    gc.collect()\n",
    "\n",
    "# trained rubert-base train (именно после сэмпла! тут уже все карты в нужном порядке разложены)\n",
    "rubert_base_oof = joblib.load(RUBERT_BASE_OOF_PATH + 'oof_preds_rubert_base.joblib')\n",
    "train['rubert_base_trained'] = rubert_base_oof\n",
    "\n",
    "rubert_base_oof_rev = joblib.load(RUBERT_BASE_OOF_REV_PATH + 'oof_preds_rubert_base_rev.joblib')\n",
    "train['rubert_base_trained_rev'] = rubert_base_oof_rev\n",
    "\n",
    "del rubert_base_oof, rubert_base_oof_rev\n",
    "gc.collect()\n",
    "\n",
    "# rubert train test tta\n",
    "if USE_MEAN_BASE_AND_REV:\n",
    "    train['rubert_base_trained_tta'] = (train['rubert_base_trained'] + train['rubert_base_trained_rev']) / 2\n",
    "    del train['rubert_base_trained'],  train['rubert_base_trained_rev']\n",
    "    gc.collect()\n",
    "\n",
    "# add rouge features (w/ tta)\n",
    "\n",
    "train_rouge = pd.read_csv(ROUGE_PATH + 'train_rouge.csv')\n",
    "test_rouge = pd.read_csv(ROUGE_PATH + 'test_rouge.csv')\n",
    "\n",
    "train_rouge = train_rouge.sort_values(by=['variantid_1', 'variantid_2']).reset_index(drop=True)\n",
    "test_rouge = test_rouge.sort_values(by=['variantid_1', 'variantid_2']).reset_index(drop=True)\n",
    "\n",
    "train['rouge_1'] = train_rouge['rouge_1']\n",
    "train['rouge_2'] = train_rouge['rouge_2']\n",
    "train['rouge_3'] = train_rouge['rouge_3']\n",
    "train['rouge_4'] = train_rouge['rouge_4']\n",
    "train['rouge_s4'] = train_rouge['rouge_s4']\n",
    "train['rouge_su4'] = train_rouge['rouge_su4']\n",
    "\n",
    "test['rouge_1'] = test_rouge['rouge_1']\n",
    "test['rouge_2'] = test_rouge['rouge_2']\n",
    "test['rouge_3'] = test_rouge['rouge_3']\n",
    "test['rouge_4'] = test_rouge['rouge_4']\n",
    "test['rouge_s4'] = test_rouge['rouge_s4']\n",
    "test['rouge_su4'] = test_rouge['rouge_su4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3517f802-f664-48b9-a5e2-3b66fa7e0b49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T12:19:55.722842Z",
     "iopub.status.busy": "2025-05-23T12:19:55.722529Z",
     "iopub.status.idle": "2025-05-23T12:20:05.608362Z",
     "shell.execute_reply": "2025-05-23T12:20:05.607301Z",
     "shell.execute_reply.started": "2025-05-23T12:19:55.722820Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train.drop(columns=['base_title_image', 'cand_title_image'], axis=1, inplace=True)\n",
    "test.drop(columns=['base_title_image', 'cand_title_image'], axis=1, inplace=True)\n",
    "train.drop(columns=['action_date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a26096b-548f-41b1-9398-b7a454fbaf49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T12:20:05.609822Z",
     "iopub.status.busy": "2025-05-23T12:20:05.609497Z",
     "iopub.status.idle": "2025-05-23T12:20:12.619705Z",
     "shell.execute_reply": "2025-05-23T12:20:12.618637Z",
     "shell.execute_reply.started": "2025-05-23T12:20:05.609799Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "cat_features = [\n",
    "    'is_same_location',\n",
    "    'is_same_region',\n",
    "    'category_level_1_match',\n",
    "    'category_level_2_match',\n",
    "    'category_level_3_match',\n",
    "    'category_level_4_match',\n",
    "    'category_level_3_fillness',\n",
    "    'category_level_4_fillness',\n",
    "    'n_images_fillness',\n",
    "    'unique_cat_1',\n",
    "    'unique_cat_2',\n",
    "    'unique_cat_3',\n",
    "    'unique_cat_4',\n",
    "]\n",
    "\n",
    "train[cat_features] = train[cat_features].astype(str)\n",
    "test[cat_features] = test[cat_features].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c750123e-fe12-4686-b151-320e6c5d4c02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T12:20:12.621135Z",
     "iopub.status.busy": "2025-05-23T12:20:12.620826Z",
     "iopub.status.idle": "2025-05-23T12:20:18.040125Z",
     "shell.execute_reply": "2025-05-23T12:20:18.039106Z",
     "shell.execute_reply.started": "2025-05-23T12:20:12.621114Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train = train.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f019e2f-a6c6-4eba-a8ac-46ea445f94e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T12:20:18.041311Z",
     "iopub.status.busy": "2025-05-23T12:20:18.041063Z",
     "iopub.status.idle": "2025-05-23T12:20:18.046206Z",
     "shell.execute_reply": "2025-05-23T12:20:18.045247Z",
     "shell.execute_reply.started": "2025-05-23T12:20:18.041292Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "features = [col for col in train.columns if col not in ['group_id', 'is_double', 'variantid_1', 'variantid_2']]\n",
    "target = 'is_double'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed82bfd5-2d66-41a5-9f5f-f9bd89effd0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T12:20:18.047569Z",
     "iopub.status.busy": "2025-05-23T12:20:18.047190Z",
     "iopub.status.idle": "2025-05-23T12:20:18.065970Z",
     "shell.execute_reply": "2025-05-23T12:20:18.064948Z",
     "shell.execute_reply.started": "2025-05-23T12:20:18.047549Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "470"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "670829eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T12:20:18.067441Z",
     "iopub.status.busy": "2025-05-23T12:20:18.067108Z",
     "iopub.status.idle": "2025-05-23T12:20:18.086695Z",
     "shell.execute_reply": "2025-05-23T12:20:18.085734Z",
     "shell.execute_reply.started": "2025-05-23T12:20:18.067414Z"
    },
    "papermill": {
     "duration": 0.14959,
     "end_time": "2025-02-01T09:24:31.627884",
     "exception": false,
     "start_time": "2025-02-01T09:24:31.478294",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "num_features = [c for c in features if not c in cat_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30221610-2e55-4da9-8375-835a3d985e8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T12:20:18.088090Z",
     "iopub.status.busy": "2025-05-23T12:20:18.087788Z",
     "iopub.status.idle": "2025-05-23T12:20:49.107085Z",
     "shell.execute_reply": "2025-05-23T12:20:49.105735Z",
     "shell.execute_reply.started": "2025-05-23T12:20:18.088068Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for col in cat_features:\n",
    "    fill_value = 'missing'\n",
    "    train[col] = train[col].fillna(fill_value)\n",
    "    test[col] = test[col].fillna(fill_value)\n",
    "\n",
    "for col in num_features:\n",
    "    train[col] = pd.to_numeric(train[col], errors='coerce')\n",
    "    test[col] = pd.to_numeric(test[col], errors='coerce')\n",
    "\n",
    "for col in num_features:\n",
    "    fill_value = train[col].median()\n",
    "    train[col] = train[col].fillna(fill_value)\n",
    "    test[col] = test[col].fillna(fill_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e5c4fcf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T12:20:49.108768Z",
     "iopub.status.busy": "2025-05-23T12:20:49.108313Z",
     "iopub.status.idle": "2025-05-23T12:21:07.676721Z",
     "shell.execute_reply": "2025-05-23T12:21:07.675717Z",
     "shell.execute_reply.started": "2025-05-23T12:20:49.108723Z"
    },
    "papermill": {
     "duration": 0.189619,
     "end_time": "2025-02-01T09:24:31.822137",
     "exception": false,
     "start_time": "2025-02-01T09:24:31.632518",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_same_location, is_same_region, category_level_1_match, category_level_2_match, category_level_3_match, category_level_4_match, category_level_3_fillness, category_level_4_fillness, n_images_fillness, unique_cat_1, unique_cat_2, unique_cat_3, unique_cat_4, "
     ]
    }
   ],
   "source": [
    "combined = pd.concat([train, test], axis=0, ignore_index=True)\n",
    "\n",
    "for c in features:\n",
    "    if c in cat_features:\n",
    "        print(f\"{c}, \", end=\"\")\n",
    "        combined[c], _ = combined[c].factorize()\n",
    "        combined[c] -= combined[c].min()\n",
    "        combined[c] = combined[c].astype(\"int32\")\n",
    "        combined[c] = combined[c].astype(\"category\")\n",
    "    else:\n",
    "        if combined[c].dtype == \"float64\":\n",
    "            combined[c] = combined[c].astype(\"float32\")\n",
    "        if combined[c].dtype == \"int64\":\n",
    "            combined[c] = combined[c].astype(\"int32\")\n",
    "    \n",
    "cat_unique = combined[cat_features].nunique().to_list()\n",
    "\n",
    "train = combined.iloc[:len(train)].reset_index(drop=True).copy()\n",
    "test = combined.iloc[len(train):].reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4776bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T12:21:07.678322Z",
     "iopub.status.busy": "2025-05-23T12:21:07.678030Z",
     "iopub.status.idle": "2025-05-23T12:22:48.586174Z",
     "shell.execute_reply": "2025-05-23T12:22:48.585162Z",
     "shell.execute_reply.started": "2025-05-23T12:21:07.678300Z"
    },
    "papermill": {
     "duration": 0.035216,
     "end_time": "2025-02-01T09:24:31.928132",
     "exception": false,
     "start_time": "2025-02-01T09:24:31.892916",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "folds = 5\n",
    "train['kfold'] = -1  \n",
    "\n",
    "target = 'is_double'\n",
    "sgkf = StratifiedGroupKFold(n_splits=folds)\n",
    "for fold, (train_idx, val_idx) in enumerate(sgkf.split(train, train[target], groups=train['group_id'])):\n",
    "    train.loc[val_idx, 'kfold'] = fold\n",
    "\n",
    "oof_metric = train[['variantid_1', 'variantid_2', 'kfold', 'group_id', 'is_double']].copy()\n",
    "oof_metric['prediction'] = 0.0\n",
    "\n",
    "oof_tabm = np.zeros(train.shape[0])\n",
    "test_tabm = np.zeros((folds, test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc950b19-6293-470e-aad7-01bd2b9e1955",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T12:22:48.587634Z",
     "iopub.status.busy": "2025-05-23T12:22:48.587196Z",
     "iopub.status.idle": "2025-05-23T12:22:52.645610Z",
     "shell.execute_reply": "2025-05-23T12:22:52.644532Z",
     "shell.execute_reply.started": "2025-05-23T12:22:48.587590Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dfsum = pd.DataFrame(train.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831fc85e-1e4d-416e-8ec7-f2893662fbbc",
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-23T12:24:06.089Z",
     "iopub.execute_input": "2025-05-23T12:22:52.646946Z",
     "iopub.status.busy": "2025-05-23T12:22:52.646643Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "cats_index = [train[features].columns.get_loc(cat) for cat in cat_features]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train[num_features] = scaler.fit_transform(train[num_features])\n",
    "test[num_features] = scaler.transform(test[num_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d32bd9",
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-23T12:24:06.090Z"
    },
    "papermill": {
     "duration": 0.051384,
     "end_time": "2025-02-01T09:24:31.991989",
     "exception": false,
     "start_time": "2025-02-01T09:24:31.940605",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_num = train[num_features].values\n",
    "X_cat = train[cat_features].values\n",
    "\n",
    "X_num_test = test[num_features].values\n",
    "X_cat_test = test[cat_features].values\n",
    "\n",
    "y = train[target].values\n",
    "\n",
    "test_dl = DataLoader(\n",
    "    TensorDataset(\n",
    "        torch.tensor(X_num_test, dtype=torch.float32),\n",
    "        torch.tensor(X_cat_test, dtype=torch.int64)\n",
    "    ), \n",
    "    batch_size=1024, \n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26338e9",
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-23T12:24:06.090Z"
    },
    "papermill": {
     "duration": 0.070661,
     "end_time": "2025-02-01T09:24:32.066776",
     "exception": false,
     "start_time": "2025-02-01T09:24:31.996115",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "n_cont_features = len(num_features)\n",
    "n_cat_features = len(cat_features)\n",
    "n_classes = 2\n",
    "cat_cardinalities = cat_unique\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9364ef53",
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-23T12:24:06.090Z"
    },
    "papermill": {
     "duration": 0.011003,
     "end_time": "2025-02-01T09:24:32.089685",
     "exception": false,
     "start_time": "2025-02-01T09:24:32.078682",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "arch_type = 'tabm-mini'\n",
    "\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4556cb9",
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-23T12:24:06.091Z"
    },
    "papermill": {
     "duration": 2751.218211,
     "end_time": "2025-02-01T10:10:23.325539",
     "exception": false,
     "start_time": "2025-02-01T09:24:32.107328",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "TRAIN = True\n",
    "MODEL_SAVE_PATH = \"tabm_models_wce\"\n",
    "\n",
    "val_wce_scores = []\n",
    "val_prauc_scores = []    \n",
    "\n",
    "if not os.path.exists(MODEL_SAVE_PATH):\n",
    "    os.makedirs(MODEL_SAVE_PATH)\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(sgkf.split(train[features], train[target], groups=train['group_id'])):\n",
    "    model_path = os.path.join(MODEL_SAVE_PATH, f\"model_fold_{i}.pt\")\n",
    "\n",
    "    best = {\n",
    "        \"val\": -math.inf,\n",
    "        \"epoch\": -1,\n",
    "    }\n",
    "    ds_true = oof_metric.loc[oof_metric.kfold == i, [\"variantid_1\", \"variantid_2\", \"is_double\", \"group_id\"]].copy().reset_index(drop=True)\n",
    "    ds_pred = oof_metric.loc[oof_metric.kfold == i, [\"variantid_1\", \"variantid_2\"]].copy().reset_index(drop=True)\n",
    "\n",
    "    X_num_train = X_num[train_index]\n",
    "    X_cat_train = X_cat[train_index]\n",
    "    y_train = y[train_index]\n",
    "\n",
    "    X_num_val = X_num[val_index]\n",
    "    X_cat_val = X_cat[val_index]\n",
    "    y_val_all = y[val_index]\n",
    "\n",
    "    train_dl = DataLoader(\n",
    "        TensorDataset(\n",
    "            torch.tensor(X_num_train, dtype=torch.float32), \n",
    "            torch.tensor(X_cat_train, dtype=torch.int64), \n",
    "            torch.tensor(y_train, dtype=torch.float32)\n",
    "        ), \n",
    "        batch_size=32, \n",
    "        shuffle=True\n",
    "    )\n",
    "    valid_dl = DataLoader(\n",
    "        TensorDataset(\n",
    "            torch.tensor(X_num_val, dtype=torch.float32), \n",
    "            torch.tensor(X_cat_val, dtype=torch.int64), \n",
    "            torch.tensor(y_val_all, dtype=torch.float32)\n",
    "        ), \n",
    "        batch_size=32, \n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    bins = rtdl_num_embeddings.compute_bins(torch.tensor(X_num_train, dtype=torch.float32))\n",
    "\n",
    "    model = Model(\n",
    "        n_num_features=n_cont_features,\n",
    "        cat_cardinalities=cat_cardinalities,\n",
    "        n_classes=n_classes,\n",
    "        backbone={\n",
    "            'type': 'MLP',\n",
    "            'n_blocks': 3 ,\n",
    "            'd_block': 512,\n",
    "            'dropout': 0.1,\n",
    "        },\n",
    "        bins=bins,\n",
    "        num_embeddings=(\n",
    "            None\n",
    "            if bins is None\n",
    "            else {\n",
    "                'type': 'PiecewiseLinearEmbeddings',\n",
    "                'd_embedding': 64,\n",
    "                'activation': True,\n",
    "                'version': 'B',\n",
    "            }\n",
    "        ),\n",
    "        arch_type=arch_type,\n",
    "        k=32,\n",
    "    ).to(device)\n",
    "\n",
    "    if TRAIN:\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            make_parameter_groups(model),\n",
    "            lr=1e-4,\n",
    "            weight_decay=1e-3 ,\n",
    "        )\n",
    "\n",
    "        patience = 15\n",
    "        early_stopping = delu.tools.EarlyStopping(patience, mode=\"max\")\n",
    "\n",
    "        for epoch in range(100):\n",
    "            model.train()   \n",
    "            with tqdm(train_dl, total=len(train_dl), leave=True) as phar:\n",
    "                for train_tensor in phar:\n",
    "                    optimizer.zero_grad()\n",
    "                    X_num_train, X_cat_train, y_train = [t.to(device) for t in train_tensor]\n",
    "\n",
    "                    output = model(X_num_train, X_cat_train).squeeze(-1)\n",
    "                    loss = loss_fn(output.flatten(0, 1), y_train.repeat_interleave(32))\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    phar.set_postfix(OrderedDict(epoch=f'{epoch+1}/{100}', loss=f'{loss.item():.6f}'))\n",
    "                    phar.update(1)\n",
    "\n",
    "            model.eval()\n",
    "            valid_pred_list = []\n",
    "            for valid_tensor in valid_dl:\n",
    "                X_num_val, X_cat_val, y_val = [t.to(device) for t in valid_tensor]\n",
    "                with torch.no_grad():\n",
    "                    output = model(X_num_val, X_cat_val).squeeze(-1)\n",
    "                valid_pred_list.append((output.mean(1).cpu().numpy(), y_val.cpu().numpy()))\n",
    "\n",
    "            valid_pred = np.concatenate([p[0] for p in valid_pred_list])\n",
    "            valid_true = np.concatenate([p[1] for p in valid_pred_list])\n",
    "            val_prauc = average_precision_score(valid_true, valid_pred)\n",
    "            \n",
    "            ds_pred[\"prediction\"] = valid_pred\n",
    "            \n",
    "            if val_prauc > best[\"val\"]:\n",
    "                print(f'{val_prauc=}')\n",
    "                best = {\"val\": val_prauc, \"epoch\": epoch, \"pred\": valid_pred}\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "\n",
    "            early_stopping.update(val_prauc)\n",
    "            if early_stopping.should_stop():\n",
    "                print(\"early stopping\")\n",
    "                break\n",
    "    else:  \n",
    "        mp = os.path.join('/kaggle/working/', mp)\n",
    "        print(f\"loading model from {mp}\")\n",
    "        model.load_state_dict(torch.load(mp))\n",
    "\n",
    "    oof_tabm[val_index] = best['pred']\n",
    "    val_prauc_scores.append(best['val'])\n",
    "\n",
    "    ds_pred[\"prediction\"] = best['pred']\n",
    "    \n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    test_pred_list = []\n",
    "    with torch.no_grad():\n",
    "        for test_tensor in test_dl:\n",
    "            X_num_test, X_cat_test = [t.to(device) for t in test_tensor]\n",
    "            output = model(X_num_test, X_cat_test).squeeze(-1)\n",
    "            test_pred_list.append(output.mean(1).cpu().numpy())\n",
    "\n",
    "    test_pred = np.concatenate([p for p in test_pred_list])\n",
    "    test_tabm[i] = test_pred\n",
    "\n",
    "    print(\" *************************************************************************************** \")\n",
    "    print(f\"fold {i+1} prauc: {val_prauc:.6f} - это просто ахуенно!\")\n",
    "    print(\"\\n\")\n",
    "    print(\" *************************************************************************************** \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59714c6",
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-23T12:24:06.091Z"
    },
    "papermill": {
     "duration": 20.527589,
     "end_time": "2025-02-01T10:11:03.867373",
     "exception": false,
     "start_time": "2025-02-01T10:10:43.339784",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"Mean Validation PRAUC: {:.6f}\".format(np.mean(val_prauc_scores)))\n",
    "print(\"OOF PRAUC: {:.6f}\".format(average_precision_score(train[target], oof_tabm)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af5416e",
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-23T12:24:06.092Z"
    },
    "papermill": {
     "duration": 20.232195,
     "end_time": "2025-02-01T10:13:05.069919",
     "exception": false,
     "start_time": "2025-02-01T10:12:44.837724",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_mean = np.mean(test_tabm, axis=0)\n",
    "submission = test[[\"variantid_1\", \"variantid_2\"]].copy()\n",
    "submission[\"prediction\"] = test_mean\n",
    "submission.to_csv(\"tabm_submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7370632,
     "isSourceIdPinned": true,
     "sourceId": 11834294,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7461624,
     "sourceId": 11873101,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 213291137,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 224194256,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2983.146053,
   "end_time": "2025-02-01T10:13:26.606154",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-01T09:23:43.460101",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
