{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec05048e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "import json\n",
    "import ast\n",
    "import pickle\n",
    "import gc\n",
    "import re\n",
    "\n",
    "from rank_bm25 import BM25Okapi\n",
    "from rouge import Rouge\n",
    "from rapidfuzz import fuzz\n",
    "import jellyfish\n",
    "import textdistance\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d975bb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pl.read_parquet('../data/merged/train_df.parquet')\n",
    "test_df = pl.read_parquet('../data/merged/test_df.parquet')\n",
    "\n",
    "print(f'{train_df.shape=}, {test_df.shape=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "710812d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_memory_usage_pl(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    print(f\"before {round(df.estimated_size('mb'), 2)=} mb\")\n",
    "    numeric_int_types = [pl.Int8, pl.Int16, pl.Int32, pl.Int64]\n",
    "    numeric_float_types = [pl.Float32, pl.Float64]    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if col_type in numeric_int_types + numeric_float_types:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if col_type in numeric_int_types:\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df = df.with_columns(df[col].cast(pl.Int8))\n",
    "                    gc.collect()\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df = df.with_columns(df[col].cast(pl.Int16))\n",
    "                    gc.collect()\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df = df.with_columns(df[col].cast(pl.Int32))\n",
    "                    gc.collect()\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df = df.with_columns(df[col].cast(pl.Int64))\n",
    "                    gc.collect()\n",
    "            elif col_type in numeric_float_types:\n",
    "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df = df.with_columns(df[col].cast(pl.Float32))\n",
    "                    gc.collect()\n",
    "                else:\n",
    "                    pass\n",
    "        elif col_type == pl.Utf8:\n",
    "            df = df.with_columns(df[col].cast(pl.Categorical))\n",
    "            gc.collect()\n",
    "        else:\n",
    "            pass\n",
    "    print(f\"after {round(df.estimated_size('mb'), 2)=} mb\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1060329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before round(df.estimated_size('mb'), 2)=28143.4 mb\n",
      "after round(df.estimated_size('mb'), 2)=11692.08 mb\n",
      "before round(df.estimated_size('mb'), 2)=4318.81 mb\n",
      "after round(df.estimated_size('mb'), 2)=2902.53 mb\n"
     ]
    }
   ],
   "source": [
    "train_df = reduce_memory_usage_pl(train_df)\n",
    "test_df = reduce_memory_usage_pl(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d4bc4d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "056bc10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = {'train_df': train_df, 'test_df': test_df}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94fe7804",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing token_sort_ratio for level 4: 100%|██████████| 2685054/2685054 [00:04<00:00, 556842.76it/s]\n",
      "Processing token_sort_ratio for level 4: 100%|██████████| 509266/509266 [00:00<00:00, 530192.29it/s]\n"
     ]
    }
   ],
   "source": [
    "# мэтч по категориям 1-4 уровня + частичный мэтч по 4 уровню\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    for i in range(1, 5):\n",
    "        df = df.with_columns(\n",
    "            pl.col(f'category_level_{i}_1').cast(pl.String).str.to_lowercase().eq(\n",
    "                pl.col(f'category_level_{i}_2').cast(pl.String).str.to_lowercase()\n",
    "            ).alias(f'category_level_{i}_match')\n",
    "        )\n",
    "        \n",
    "        if i == 4:\n",
    "            def calc_token_sort_ratio(row):\n",
    "                val1 = str(row[f'category_level_4_1'])\n",
    "                val2 = str(row[f'category_level_4_2'])\n",
    "                return fuzz.token_sort_ratio(val1, val2) / 100\n",
    "            \n",
    "            total = len(df)\n",
    "            pbar = tqdm(total=total, desc=f'Processing token_sort_ratio for level {i}')\n",
    "            \n",
    "            def token_sort_with_progress(row):\n",
    "                pbar.update(1)\n",
    "                return calc_token_sort_ratio(row)\n",
    "            \n",
    "            df = df.with_columns(\n",
    "                pl.struct([f'category_level_4_1', f'category_level_4_2'])\n",
    "                .map_elements(token_sort_with_progress, return_dtype=pl.Float64)\n",
    "                .alias(f'category_level_4_token_sort_ratio_match')\n",
    "            )\n",
    "            pbar.close()\n",
    "            \n",
    "    if name == 'train_df':\n",
    "        train_df = df\n",
    "    else:\n",
    "        test_df = df\n",
    "        \n",
    "    dataframes = {'train_df': train_df, 'test_df': test_df}\n",
    "        \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a98f5478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# всего совпадений по категориям\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    df = df.with_columns(\n",
    "        (pl.col('category_level_1_match').fill_null(False).cast(pl.Int64) + \n",
    "         pl.col('category_level_2_match').fill_null(False).cast(pl.Int64) + \n",
    "         pl.col('category_level_3_match').fill_null(False).cast(pl.Int64) + \n",
    "         pl.col('category_level_4_match').fill_null(False).cast(pl.Int64))\n",
    "        .alias('category_total_matches')\n",
    "    )\n",
    "    \n",
    "    if name == 'train_df':\n",
    "        train_df = df\n",
    "    else:\n",
    "        test_df = df\n",
    "        \n",
    "    dataframes = {'train_df': train_df, 'test_df': test_df}\n",
    "        \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bfc0d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing price ratios: 100%|██████████| 2685054/2685054 [00:04<00:00, 646749.29it/s] \n",
      "Processing n_images ratios: 100%|██████████| 2685054/2685054 [00:04<00:00, 641755.52it/s] \n",
      "Processing name_tokens_len ratios: 100%|██████████| 2685054/2685054 [00:04<00:00, 629052.23it/s] \n",
      "Processing description_tokens_len ratios: 100%|██████████| 2685054/2685054 [00:04<00:00, 611046.96it/s] \n",
      "Processing name_en_tokens_len ratios: 100%|██████████| 2685054/2685054 [00:04<00:00, 602786.71it/s] \n",
      "Processing description_en_tokens_len ratios: 100%|██████████| 2685054/2685054 [00:04<00:00, 616463.18it/s] \n",
      "Processing name_mix_tokens_len ratios: 100%|██████████| 2685054/2685054 [00:04<00:00, 616450.76it/s] \n",
      "Processing description_mix_tokens_len ratios: 100%|██████████| 2685054/2685054 [00:04<00:00, 604195.24it/s] \n",
      "Processing attr_keys_len ratios: 100%|██████████| 2685054/2685054 [00:04<00:00, 629671.28it/s] \n",
      "Processing attr_vals_len ratios: 100%|██████████| 2685054/2685054 [00:04<00:00, 622498.70it/s] \n",
      "Processing name_tokens_w_digits ratios: 100%|██████████| 2685054/2685054 [00:06<00:00, 410173.05it/s]\n",
      "Processing description_tokens_w_digits ratios: 100%|██████████| 2685054/2685054 [00:12<00:00, 223170.83it/s]\n",
      "Processing price ratios: 100%|██████████| 509266/509266 [00:00<00:00, 655830.33it/s]\n",
      "Processing n_images ratios: 100%|██████████| 509266/509266 [00:00<00:00, 640861.13it/s]\n",
      "Processing name_tokens_len ratios: 100%|██████████| 509266/509266 [00:00<00:00, 620383.15it/s]\n",
      "Processing description_tokens_len ratios: 100%|██████████| 509266/509266 [00:00<00:00, 596401.74it/s]\n",
      "Processing name_en_tokens_len ratios: 100%|██████████| 509266/509266 [00:00<00:00, 600050.96it/s]\n",
      "Processing description_en_tokens_len ratios: 100%|██████████| 509266/509266 [00:00<00:00, 607945.82it/s]\n",
      "Processing name_mix_tokens_len ratios: 100%|██████████| 509266/509266 [00:00<00:00, 606406.11it/s]\n",
      "Processing description_mix_tokens_len ratios: 100%|██████████| 509266/509266 [00:00<00:00, 604639.27it/s]\n",
      "Processing attr_keys_len ratios: 100%|██████████| 509266/509266 [00:00<00:00, 635043.49it/s]\n",
      "Processing attr_vals_len ratios: 100%|██████████| 509266/509266 [00:00<00:00, 632439.52it/s]\n",
      "Processing name_tokens_w_digits ratios: 100%|██████████| 509266/509266 [00:01<00:00, 434686.68it/s]\n",
      "Processing description_tokens_w_digits ratios: 100%|██████████| 509266/509266 [00:01<00:00, 267698.13it/s]\n"
     ]
    }
   ],
   "source": [
    "# отношения длин для всего подряд (левые и правые)\n",
    "\n",
    "num_cols = [\n",
    "    'price', 'n_images', \n",
    "    'name_tokens_len', 'description_tokens_len', \n",
    "    'name_en_tokens_len', 'description_en_tokens_len',\n",
    "    'name_mix_tokens_len', 'description_mix_tokens_len',\n",
    "    'attr_keys_len', 'attr_vals_len',\n",
    "]\n",
    "\n",
    "str_cols = ['name_tokens_w_digits', 'description_tokens_w_digits']\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    for col in num_cols:\n",
    "        total = len(df)\n",
    "        pbar = tqdm(total=total, desc=f\"Processing {col} ratios\")\n",
    "\n",
    "        def ratio_left(row):\n",
    "            pbar.update(1)\n",
    "            val2 = row[f'{col}_2']\n",
    "            val1 = row[f'{col}_1']\n",
    "            \n",
    "            if val2 is None or val2 == 0 or val1 is None:\n",
    "                return 0.0\n",
    "            \n",
    "            return float(val1) / float(val2)\n",
    "\n",
    "        def ratio_right(row):\n",
    "            val2 = row[f'{col}_2']\n",
    "            val1 = row[f'{col}_1']\n",
    "            \n",
    "            if val1 is None or val1 == 0 or val2 is None:\n",
    "                return 0.0\n",
    "                \n",
    "            return float(val2) / float(val1)\n",
    "\n",
    "        df = df.with_columns(\n",
    "            pl.struct([f'{col}_1', f'{col}_2']).map_elements(\n",
    "                ratio_left, return_dtype=pl.Float64\n",
    "            ).alias(f'{col}_ratio_left'),\n",
    "            \n",
    "            pl.struct([f'{col}_1', f'{col}_2']).map_elements(\n",
    "                ratio_right, return_dtype=pl.Float64\n",
    "            ).alias(f'{col}_ratio_right')\n",
    "        )\n",
    "        pbar.close()\n",
    "\n",
    "    for col in str_cols:\n",
    "        total = len(df)\n",
    "        pbar = tqdm(total=total, desc=f\"Processing {col} ratios\")\n",
    "\n",
    "        def str_ratio_left(row):\n",
    "            pbar.update(1)\n",
    "            val1 = row[f'{col}_1']\n",
    "            val2 = row[f'{col}_2']\n",
    "            \n",
    "            if val1 is None:\n",
    "                len1 = 0\n",
    "            else:\n",
    "                len1 = len(str(val1).split())\n",
    "                \n",
    "            if val2 is None:\n",
    "                len2 = 0\n",
    "            else:\n",
    "                len2 = len(str(val2).split())\n",
    "                \n",
    "            if len2 == 0:\n",
    "                return 0.0\n",
    "                \n",
    "            return float(len1) / float(len2)\n",
    "\n",
    "        def str_ratio_right(row):\n",
    "            val1 = row[f'{col}_1']\n",
    "            val2 = row[f'{col}_2']\n",
    "            \n",
    "            if val1 is None:\n",
    "                len1 = 0\n",
    "            else:\n",
    "                len1 = len(str(val1).split())\n",
    "                \n",
    "            if val2 is None:\n",
    "                len2 = 0\n",
    "            else:\n",
    "                len2 = len(str(val2).split())\n",
    "                \n",
    "            if len1 == 0:\n",
    "                return 0.0\n",
    "                \n",
    "            return float(len2) / float(len1)\n",
    "\n",
    "        df = df.with_columns(\n",
    "            pl.struct([f'{col}_1', f'{col}_2']).map_elements(\n",
    "                str_ratio_left, return_dtype=pl.Float64\n",
    "            ).alias(f'{col}_ratio_left'),\n",
    "            \n",
    "            pl.struct([f'{col}_1', f'{col}_2']).map_elements(\n",
    "                str_ratio_right, return_dtype=pl.Float64\n",
    "            ).alias(f'{col}_ratio_right')\n",
    "        )\n",
    "        pbar.close()\n",
    "        \n",
    "    if name == 'train_df':\n",
    "        train_df = df\n",
    "    else:\n",
    "        test_df = df\n",
    "        \n",
    "    dataframes = {'train_df': train_df, 'test_df': test_df}\n",
    "        \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afa4839",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing abs_diff for price: 100%|██████████| 2685054/2685054 [00:03<00:00, 802840.22it/s]\n",
      "Processing abs_diff for n_images: 100%|██████████| 2685054/2685054 [00:03<00:00, 851379.60it/s]\n",
      "Processing abs_diff for attr_keys_len: 100%|██████████| 2685054/2685054 [00:03<00:00, 885675.72it/s]\n",
      "Processing abs_diff for attr_vals_len: 100%|██████████| 2685054/2685054 [00:02<00:00, 900578.32it/s]\n",
      "Processing abs_diff for price: 100%|██████████| 509266/509266 [00:00<00:00, 809163.03it/s]\n",
      "Processing abs_diff for n_images: 100%|██████████| 509266/509266 [00:00<00:00, 825872.35it/s]\n",
      "Processing abs_diff for attr_keys_len: 100%|██████████| 509266/509266 [00:00<00:00, 877200.81it/s]\n",
      "Processing abs_diff for attr_vals_len: 100%|██████████| 509266/509266 [00:00<00:00, 889748.07it/s]\n"
     ]
    }
   ],
   "source": [
    "# разницы длин\n",
    "\n",
    "def abs_len_diff_features(df, cols):\n",
    "    for col in cols:\n",
    "        total = len(df)\n",
    "        pbar = tqdm(total=total, desc=f\"Processing abs_diff for {col}\")\n",
    "\n",
    "        def abs_len_diff(row):\n",
    "            pbar.update(1)\n",
    "            val1 = row[f'{col}_1']\n",
    "            val2 = row[f'{col}_2']\n",
    "            if val1 is None or val2 is None:\n",
    "                return 0\n",
    "            v1 = int(val1)\n",
    "            v2 = int(val2)\n",
    "            return abs(v1 - v2)\n",
    "\n",
    "        df = df.with_columns(\n",
    "            pl.struct([f'{col}_1', f'{col}_2'])\n",
    "            .map_elements(abs_len_diff, return_dtype=pl.Int64)\n",
    "            .alias(f'{col}_abs_diff')\n",
    "        )\n",
    "        pbar.close()\n",
    "        \n",
    "    return df\n",
    "\n",
    "def abs_len_sqdiff_features(df, cols):\n",
    "    for col in cols:\n",
    "        total = len(df)\n",
    "        pbar = tqdm(total=total, desc=f\"Processing sq_diff for {col}\")\n",
    "\n",
    "        def sq_len_diff(row):\n",
    "            pbar.update(1)\n",
    "            val1 = row[f'{col}_1']\n",
    "            val2 = row[f'{col}_2']\n",
    "            if val1 is None or val2 is None:\n",
    "                return 0\n",
    "            v1 = int(val1)\n",
    "            v2 = int(val2)\n",
    "            return (v1 - v2) ** 2\n",
    "\n",
    "        df = df.with_columns(\n",
    "            pl.struct([f'{col}_1', f'{col}_2'])\n",
    "            .map_elements(sq_len_diff, return_dtype=pl.Int64)\n",
    "            .alias(f'{col}_sq_diff')\n",
    "        )\n",
    "        pbar.close()\n",
    "        \n",
    "    return df\n",
    "\n",
    "diff_cols = ['price', 'n_images', 'attr_keys_len', 'attr_vals_len']\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    df = abs_len_diff_features(df, diff_cols)\n",
    "    df = abs_len_sqdiff_features(df, diff_cols)\n",
    "    \n",
    "    if name == 'train_df':\n",
    "        train_df = df\n",
    "    else:\n",
    "        test_df = df\n",
    "        \n",
    "    dataframes = {'train_df': train_df, 'test_df': test_df}\n",
    "        \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "825fd1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# полнота некоторых столбцов с пропусками\n",
    "\n",
    "def fillness(df: pl.DataFrame, col_name: str) -> pl.DataFrame:\n",
    "    condition_both = (pl.col(f'{col_name}_1').is_not_null() & \n",
    "                      pl.col(f'{col_name}_2').is_not_null())\n",
    "    condition_none = (pl.col(f'{col_name}_1').is_null() & \n",
    "                      pl.col(f'{col_name}_2').is_null())\n",
    "    \n",
    "    df = df.with_columns(\n",
    "        pl.when(condition_both).then(pl.lit('both'))\n",
    "        .when(condition_none).then(pl.lit('none'))\n",
    "        .otherwise(pl.lit('only one'))\n",
    "        .alias(f'{col_name}_fillness')\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    df = fillness(df, 'category_level_3')\n",
    "    df = fillness(df, 'category_level_4')\n",
    "    df = fillness(df, 'n_images')\n",
    "    \n",
    "    if name == 'train_df':\n",
    "        train_df = df\n",
    "    else:\n",
    "        test_df = df\n",
    "        \n",
    "    dataframes = {'train_df': train_df, 'test_df': test_df}\n",
    "        \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd10b2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# фичи на антисловах\n",
    "\n",
    "with open('../data/preprocessed/filtered_anti_words.pkl', 'rb') as file:\n",
    "    filtered_anti_words = pickle.load(file)\n",
    "\n",
    "N_TOP_ANTIWORD = 100\n",
    "top_anti_words = set([w[0] for w in filtered_anti_words.most_common(N_TOP_ANTIWORD)])\n",
    "\n",
    "def calc_anti_words_values(row: dict) -> float:\n",
    "    name1, name2 = row['name_1'], row['name_2']\n",
    "    words1 = set(re.findall(r'([a-z]+)', name1.lower()))\n",
    "    words2 = set(re.findall(r'([a-z]+)', name2.lower()))\n",
    "    \n",
    "    if not (words1 | words2):\n",
    "        return 0.0\n",
    "        \n",
    "    xor_words = words1.symmetric_difference(words2)\n",
    "    intersection = xor_words & top_anti_words\n",
    "    return len(intersection) / max(len(words1), len(words2))\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    df = df.with_columns(\n",
    "        pl.struct(['name_1', 'name_2'])\n",
    "        .map_elements(calc_anti_words_values, return_dtype=pl.Float64)\n",
    "        .alias('anti_words_values')\n",
    "    )\n",
    "        \n",
    "    if name == 'train_df':\n",
    "        train_df = df\n",
    "    else:\n",
    "        test_df = df\n",
    "        \n",
    "    dataframes = {'train_df': train_df, 'test_df': test_df}\n",
    "        \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa9edaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating avg_fully_eq_attributes: 100%|██████████| 2685054/2685054 [03:18<00:00, 13513.34it/s]\n",
      "Calculating avg_fully_eq_attributes: 100%|██████████| 509266/509266 [00:49<00:00, 10192.64it/s]\n"
     ]
    }
   ],
   "source": [
    "# совпадения для словаря атрибутов\n",
    "\n",
    "def avg_fully_eq_attributes(d1, d2):\n",
    "    if d1 is None or d2 is None:\n",
    "        return None\n",
    "    try:\n",
    "        d1 = ast.literal_eval(d1)\n",
    "        d2 = ast.literal_eval(d2)\n",
    "    except Exception:\n",
    "        print('bad!!!')\n",
    "        return None\n",
    "    keys = set(d1) & set(d2)\n",
    "    metrics = []\n",
    "    for key in keys:\n",
    "        metrics.append(set(d1[key]) == set(d2[key]))\n",
    "    if len(metrics) == 0:\n",
    "        return None\n",
    "    return np.mean(metrics)\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    total = len(df)\n",
    "    pbar = tqdm(total=total, desc='Calculating avg_fully_eq_attributes')\n",
    "\n",
    "    def apply_avg_fully_eq_attributes(row):\n",
    "        pbar.update(1)\n",
    "        return avg_fully_eq_attributes(\n",
    "            row['characteristic_attributes_mapping_1'], \n",
    "            row['characteristic_attributes_mapping_2']\n",
    "        )\n",
    "\n",
    "    df = df.with_columns(\n",
    "        pl.struct(['characteristic_attributes_mapping_1', 'characteristic_attributes_mapping_2'])\n",
    "        .map_elements(apply_avg_fully_eq_attributes, return_dtype=pl.Float64)\n",
    "        .alias('attributes_values_avg_fully_eq')\n",
    "    )\n",
    "    pbar.close()\n",
    "        \n",
    "    if name == 'train_df':\n",
    "        train_df = df\n",
    "    else:\n",
    "        test_df = df\n",
    "        \n",
    "    dataframes = {'train_df': train_df, 'test_df': test_df}\n",
    "        \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebed4a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing name similarity metrics: 100%|██████████| 2685054/2685054 [05:34<00:00, 8026.83it/s] \n",
      "Processing name_norm similarity metrics: 100%|██████████| 2685054/2685054 [05:22<00:00, 8320.28it/s] \n",
      "Processing name_en similarity metrics: 100%|██████████| 2685054/2685054 [01:33<00:00, 28758.05it/s] \n",
      "Processing name_mix similarity metrics: 100%|██████████| 2685054/2685054 [00:59<00:00, 45383.99it/s] \n",
      "Processing description_mix similarity metrics: 100%|██████████| 2685054/2685054 [05:53<00:00, 7595.57it/s] \n",
      "Processing name_tokens_w_digits similarity metrics: 100%|██████████| 2685054/2685054 [01:07<00:00, 39957.70it/s] \n",
      "Processing description_tokens_w_digits similarity metrics: 100%|██████████| 2685054/2685054 [14:05<00:00, 3174.02it/s] \n",
      "Processing description similarity: 100%|██████████| 2685054/2685054 [33:35<00:00, 1332.22it/s]\n",
      "Processing name similarity metrics: 100%|██████████| 509266/509266 [00:58<00:00, 8717.13it/s] \n",
      "Processing name_norm similarity metrics: 100%|██████████| 509266/509266 [00:56<00:00, 9076.68it/s] \n",
      "Processing name_en similarity metrics: 100%|██████████| 509266/509266 [00:12<00:00, 39913.47it/s]\n",
      "Processing name_mix similarity metrics: 100%|██████████| 509266/509266 [00:07<00:00, 70177.76it/s] \n",
      "Processing description_mix similarity metrics: 100%|██████████| 509266/509266 [01:14<00:00, 6838.04it/s] \n",
      "Processing name_tokens_w_digits similarity metrics: 100%|██████████| 509266/509266 [00:08<00:00, 60200.74it/s] \n",
      "Processing description_tokens_w_digits similarity metrics: 100%|██████████| 509266/509266 [02:15<00:00, 3755.94it/s] \n",
      "Processing description similarity: 100%|██████████| 509266/509266 [04:53<00:00, 1734.81it/s]\n"
     ]
    }
   ],
   "source": [
    "# частичные мэтчи по названиям и описаниям\n",
    "\n",
    "def apply_string_metrics(df, col, include_extra_metrics=True):\n",
    "    total = len(df)\n",
    "    pbar = tqdm(total=total, desc=f\"Processing {col} similarity metrics\")\n",
    "    def token_sort_ratio(row):\n",
    "        pbar.update(1)\n",
    "        val1 = row[f'{col}_1']\n",
    "        val2 = row[f'{col}_2']\n",
    "        if val1 is None or val2 is None:\n",
    "            return 0.0\n",
    "        return fuzz.token_sort_ratio(str(val1), str(val2)) / 100\n",
    "    \n",
    "    def token_set_ratio(row):\n",
    "        val1 = row[f'{col}_1']\n",
    "        val2 = row[f'{col}_2']\n",
    "        if val1 is None or val2 is None:\n",
    "            return 0.0\n",
    "        return fuzz.token_set_ratio(str(val1), str(val2)) / 100\n",
    "    \n",
    "    def jaro_winkler_similarity(row):\n",
    "        val1 = row[f'{col}_1']\n",
    "        val2 = row[f'{col}_2']\n",
    "        if val1 is None or val2 is None:\n",
    "            return 0.0\n",
    "        return jellyfish.jaro_winkler_similarity(str(val1), str(val2))\n",
    "    \n",
    "    def dice_similarity(row):\n",
    "        val1 = row[f'{col}_1']\n",
    "        val2 = row[f'{col}_2']\n",
    "        if val1 is None or val2 is None:\n",
    "            return 0.0\n",
    "        return textdistance.dice(str(val1), str(val2))\n",
    "    \n",
    "    def tanimoto_similarity(row):\n",
    "        val1 = row[f'{col}_1']\n",
    "        val2 = row[f'{col}_2']\n",
    "        if val1 is None or val2 is None:\n",
    "            return 0.0\n",
    "        return textdistance.tanimoto(str(val1), str(val2))\n",
    "    \n",
    "    def sorensen_similarity(row):\n",
    "        val1 = row[f'{col}_1']\n",
    "        val2 = row[f'{col}_2']\n",
    "        if val1 is None or val2 is None:\n",
    "            return 0.0\n",
    "        return textdistance.sorensen(str(val1), str(val2))\n",
    "    \n",
    "    new_columns = [\n",
    "        pl.struct([f'{col}_1', f'{col}_2']).map_elements(\n",
    "            token_sort_ratio, return_dtype=pl.Float64\n",
    "        ).alias(f'{col}_token_sort_ratio'),\n",
    "        \n",
    "        pl.struct([f'{col}_1', f'{col}_2']).map_elements(\n",
    "            token_set_ratio, return_dtype=pl.Float64\n",
    "        ).alias(f'{col}_token_set_ratio'),\n",
    "        \n",
    "        pl.struct([f'{col}_1', f'{col}_2']).map_elements(\n",
    "            jaro_winkler_similarity, return_dtype=pl.Float64\n",
    "        ).alias(f'{col}_jaro_winkler_similarity'),\n",
    "        \n",
    "        pl.struct([f'{col}_1', f'{col}_2']).map_elements(\n",
    "            dice_similarity, return_dtype=pl.Float64\n",
    "        ).alias(f'{col}_dice'),\n",
    "        \n",
    "        pl.struct([f'{col}_1', f'{col}_2']).map_elements(\n",
    "            tanimoto_similarity, return_dtype=pl.Float64\n",
    "        ).alias(f'{col}_tanimoto'),\n",
    "        \n",
    "        pl.struct([f'{col}_1', f'{col}_2']).map_elements(\n",
    "            sorensen_similarity, return_dtype=pl.Float64\n",
    "        ).alias(f'{col}_sorensen')\n",
    "    ]\n",
    "    \n",
    "    if include_extra_metrics and 'attr' not in col:\n",
    "        def damerau_levenshtein_distance(row):\n",
    "            val1 = row[f'{col}_1']\n",
    "            val2 = row[f'{col}_2']\n",
    "            if val1 is None or val2 is None:\n",
    "                return 0\n",
    "            return jellyfish.damerau_levenshtein_distance(str(val1), str(val2))\n",
    "        \n",
    "        def wratio_similarity(row):\n",
    "            val1 = row[f'{col}_1']\n",
    "            val2 = row[f'{col}_2']\n",
    "            if val1 is None or val2 is None:\n",
    "                return 0.0\n",
    "            return fuzz.WRatio(str(val1), str(val2)) / 100\n",
    "        \n",
    "        new_columns.extend([\n",
    "            pl.struct([f'{col}_1', f'{col}_2']).map_elements(\n",
    "                damerau_levenshtein_distance, return_dtype=pl.Int64\n",
    "            ).alias(f'{col}_damerau_levenshtein_distance'),\n",
    "            \n",
    "            pl.struct([f'{col}_1', f'{col}_2']).map_elements(\n",
    "                wratio_similarity, return_dtype=pl.Float64\n",
    "            ).alias(f'{col}_WRatio')\n",
    "        ])\n",
    "    \n",
    "    df = df.with_columns(new_columns)\n",
    "    pbar.close()\n",
    "    return df\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    for col in ('name', 'name_norm', 'name_en', 'name_mix', 'description_mix', 'name_tokens_w_digits', 'description_tokens_w_digits'):\n",
    "        df = apply_string_metrics(df, col)\n",
    "\n",
    "    total = len(df)\n",
    "    pbar = tqdm(total=total, desc=\"Processing description similarity\")\n",
    "\n",
    "    def description_jaro_winkler(row):\n",
    "        pbar.update(1)\n",
    "        val1 = row['description_1']\n",
    "        val2 = row['description_2']\n",
    "        if val1 is None or val2 is None:\n",
    "            return 0.0\n",
    "        return jellyfish.jaro_winkler_similarity(str(val1), str(val2))\n",
    "    \n",
    "    def description_token_sort_ratio(row):\n",
    "        val1 = row['description_1']\n",
    "        val2 = row['description_2']\n",
    "        if val1 is None or val2 is None:\n",
    "            return 0.0\n",
    "        return fuzz.token_sort_ratio(str(val1), str(val2)) / 100\n",
    "    \n",
    "    def description_token_set_ratio(row):\n",
    "        val1 = row['description_1']\n",
    "        val2 = row['description_2']\n",
    "        if val1 is None or val2 is None:\n",
    "            return 0.0\n",
    "        return fuzz.token_set_ratio(str(val1), str(val2)) / 100\n",
    "    \n",
    "    df = df.with_columns(\n",
    "        pl.struct(['description_1', 'description_2']).map_elements(\n",
    "            description_jaro_winkler, return_dtype=pl.Float64\n",
    "        ).alias('description_jaro_winkler_similarity')\n",
    "    )\n",
    "    df = df.with_columns(\n",
    "        pl.struct(['description_1', 'description_2']).map_elements(\n",
    "            description_token_sort_ratio, return_dtype=pl.Float64\n",
    "        ).alias('description_token_sort_ratio')\n",
    "    )\n",
    "    df = df.with_columns(\n",
    "        pl.struct(['description_1', 'description_2']).map_elements(\n",
    "            description_token_set_ratio, return_dtype=pl.Float64\n",
    "        ).alias('description_token_set_ratio')\n",
    "    )\n",
    "    pbar.close()\n",
    "        \n",
    "    if name == 'train_df':\n",
    "        train_df = df\n",
    "    else:\n",
    "        test_df = df\n",
    "        \n",
    "    dataframes = {'train_df': train_df, 'test_df': test_df}\n",
    "        \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e533f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating top attribute matches for level 1: 100%|██████████| 2685054/2685054 [01:15<00:00, 35374.06it/s]\n",
      "Generating top attribute matches for level 2: 100%|██████████| 2685054/2685054 [01:15<00:00, 35568.43it/s]\n",
      "Generating top attribute matches for level 3: 100%|██████████| 2685054/2685054 [01:09<00:00, 38386.70it/s]\n",
      "Generating top attribute matches for level 4: 100%|██████████| 2685054/2685054 [00:59<00:00, 45288.64it/s]\n",
      "Generating top attribute matches for level 1: 100%|██████████| 509266/509266 [00:15<00:00, 32841.93it/s]\n",
      "Generating top attribute matches for level 2: 100%|██████████| 509266/509266 [00:15<00:00, 32805.19it/s]\n",
      "Generating top attribute matches for level 3: 100%|██████████| 509266/509266 [00:13<00:00, 36429.04it/s]\n",
      "Generating top attribute matches for level 4: 100%|██████████| 509266/509266 [00:11<00:00, 44627.55it/s]\n"
     ]
    }
   ],
   "source": [
    "# совпадения топ-атрибутов по категориям\n",
    "\n",
    "pop_characts_tf_idf = {}\n",
    "for level in range(1, 5):\n",
    "    with open(f'../data/preprocessed/pop_characts_tf_idf_level_{level}.pkl', 'rb') as file:\n",
    "        pop_characts_tf_idf[level] = pickle.load(file)\n",
    "\n",
    "TOP_N_characts = 75\n",
    "\n",
    "def generate_top_attribute_matches(df, level):\n",
    "    total = len(df)\n",
    "    pbar = tqdm(total=total, desc=f\"Generating top attribute matches for level {level}\")\n",
    "    \n",
    "    def code_top_tf_idf_characteristics(row):\n",
    "        pbar.update(1)\n",
    "        cat_1 = row[f'category_level_{level}_1']\n",
    "        cat_2 = row[f'category_level_{level}_2']\n",
    "        attr_1 = row['characteristic_attributes_mapping_1']\n",
    "        attr_2 = row['characteristic_attributes_mapping_2']\n",
    "        \n",
    "        result = [0.0] * TOP_N_characts\n",
    "        \n",
    "        if attr_1 is None or attr_2 is None or cat_1 != cat_2 or cat_1 not in pop_characts_tf_idf[level]:\n",
    "            return result\n",
    "        \n",
    "        try:\n",
    "            attr_1 = json.loads(attr_1)\n",
    "            attr_2 = json.loads(attr_2)\n",
    "        except (json.JSONDecodeError, TypeError):\n",
    "            print('bad!!!')\n",
    "            return result\n",
    "        \n",
    "        top_pop_characts = [i[1] for i in pop_characts_tf_idf[level][cat_1][:TOP_N_characts]]\n",
    "        \n",
    "        for i, cat_name in enumerate(top_pop_characts):\n",
    "            if i >= TOP_N_characts:\n",
    "                break\n",
    "            if cat_name in attr_1 and cat_name in attr_2:\n",
    "                if attr_1[cat_name] == attr_2[cat_name]:\n",
    "                    result[i] = 1.0\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    df = df.with_columns(\n",
    "        pl.struct([\n",
    "            f'category_level_{level}_1', \n",
    "            f'category_level_{level}_2', \n",
    "            'characteristic_attributes_mapping_1', \n",
    "            'characteristic_attributes_mapping_2'\n",
    "        ]).map_elements(code_top_tf_idf_characteristics, return_dtype=pl.List(pl.Float64))\n",
    "        .alias(f'top_attr_match_list_lvl{level}')\n",
    "    )\n",
    "    \n",
    "    new_cols = []\n",
    "    new_col_names = []\n",
    "    for i in range(TOP_N_characts):\n",
    "        col_name = f'top_{i}_attr_match_lvl{level}'\n",
    "        new_cols.append(\n",
    "            pl.col(f'top_attr_match_list_lvl{level}').list.get(i).alias(col_name)\n",
    "        )\n",
    "        new_col_names.append(col_name)\n",
    "    \n",
    "    df = df.with_columns(new_cols)\n",
    "    \n",
    "    df = df.with_columns(\n",
    "        pl.sum_horizontal([pl.col(c).cast(pl.Float64) for c in new_col_names])\n",
    "        .alias(f'total_top_attribute_matches_lvl{level}')\n",
    "    )\n",
    "    \n",
    "    df = df.drop(f'top_attr_match_list_lvl{level}')\n",
    "    pbar.close()\n",
    "    \n",
    "    return df\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    for level in range(1, 5):\n",
    "        df = generate_top_attribute_matches(df, level)\n",
    "        gc.collect() \n",
    "    \n",
    "    if name == 'train_df':\n",
    "        train_df = df\n",
    "    else:\n",
    "        test_df = df\n",
    "        \n",
    "    dataframes = {'train_df': train_df, 'test_df': test_df}\n",
    "        \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddc9828",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating LCP and LCS for name_norm: 100%|██████████| 2685054/2685054 [00:32<00:00, 138695.93it/s]"
     ]
    }
   ],
   "source": [
    "# lcp&lcs для названий\n",
    "\n",
    "def longest_common_prefix(str1, str2):\n",
    "    if str1 is None or str2 is None:\n",
    "        return None\n",
    "    \n",
    "    min_len = min(len(str1), len(str2))\n",
    "    prefix_len = 0\n",
    "    \n",
    "    for i in range(min_len):\n",
    "        if str1[i] == str2[i]:\n",
    "            prefix_len += 1\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return prefix_len / min_len if min_len != 0 else 0\n",
    "\n",
    "def longest_common_subsequence(str1, str2):\n",
    "    if str1 is None or str2 is None:\n",
    "        return None\n",
    "    \n",
    "    len1, len2 = len(str1), len(str2)\n",
    "    dp = [[0] * (len2 + 1) for _ in range(len1 + 1)]\n",
    "    \n",
    "    for i in range(1, len1 + 1):\n",
    "        for j in range(1, len2 + 1):\n",
    "            if str1[i - 1] == str2[j - 1]:\n",
    "                dp[i][j] = dp[i - 1][j - 1] + 1\n",
    "            else:\n",
    "                dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])\n",
    "    \n",
    "    lcs_len = dp[len1][len2]\n",
    "    return lcs_len / max(len1, len2) if max(len1, len2) != 0 else 0\n",
    "\n",
    "for name, df in dataframes.items():      \n",
    "    for col in ('name_norm', 'description_norm'):\n",
    "        total = len(df)\n",
    "        pbar = tqdm(total=total, desc=f'Calculating LCP and LCS for {col}')\n",
    "        \n",
    "        def lcp_with_progress(row):\n",
    "            pbar.update(1)\n",
    "            return longest_common_prefix(row[f'{col}_1'], row[f'{col}_2'])\n",
    "        \n",
    "        def lcs_with_progress(row):\n",
    "            return longest_common_subsequence(row[f'{col}_1'], row[f'{col}_2'])\n",
    "        \n",
    "        df = df.with_columns([\n",
    "            pl.struct([f'{col}_1', f'{col}_2'])\n",
    "            .map_elements(lcp_with_progress, return_dtype=pl.Float64)\n",
    "            .alias(f'{col}_lcp'),\n",
    "            \n",
    "            pl.struct([f'{col}_1', f'{col}_2'])\n",
    "            .map_elements(lcs_with_progress, return_dtype=pl.Float64)\n",
    "            .alias(f'{col}_lcs')\n",
    "        ])\n",
    "        pbar.close()\n",
    "        \n",
    "    if name == 'train_df':\n",
    "        train_df = df\n",
    "    else:\n",
    "        test_df = df\n",
    "        \n",
    "    dataframes = {'train_df': train_df, 'test_df': test_df}\n",
    "        \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db55ae6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing attr_keys jaccard and overlap scores: 100%|██████████| 2685054/2685054 [00:20<00:00, 129141.80it/s]\n",
      "Processing attr_vals jaccard and overlap scores: 100%|██████████| 2685054/2685054 [00:21<00:00, 122630.63it/s]\n",
      "Processing units_name jaccard and overlap scores: 100%|██████████| 2685054/2685054 [00:09<00:00, 283760.62it/s]\n",
      "Processing units_desc jaccard and overlap scores: 100%|██████████| 2685054/2685054 [00:14<00:00, 185073.14it/s]\n",
      "Processing brands_name jaccard and overlap scores: 100%|██████████| 2685054/2685054 [00:11<00:00, 240986.61it/s]\n",
      "Processing brands_desc jaccard and overlap scores: 100%|██████████| 2685054/2685054 [00:17<00:00, 152770.76it/s]\n",
      "Processing attr_keys jaccard and overlap scores: 100%|██████████| 509266/509266 [00:04<00:00, 105241.30it/s]\n",
      "Processing attr_vals jaccard and overlap scores: 100%|██████████| 509266/509266 [00:05<00:00, 93998.43it/s] \n",
      "Processing units_name jaccard and overlap scores: 100%|██████████| 509266/509266 [00:01<00:00, 286825.84it/s]\n",
      "Processing units_desc jaccard and overlap scores: 100%|██████████| 509266/509266 [00:02<00:00, 203269.15it/s]\n",
      "Processing brands_name jaccard and overlap scores: 100%|██████████| 509266/509266 [00:02<00:00, 239728.58it/s]\n",
      "Processing brands_desc jaccard and overlap scores: 100%|██████████| 509266/509266 [00:03<00:00, 168679.16it/s]\n"
     ]
    }
   ],
   "source": [
    "# сходство для списков\n",
    "\n",
    "def jaccard_similarity(list1, list2):\n",
    "    if list1 is None or list2 is None:\n",
    "        return None\n",
    "    set1 = set(list1)\n",
    "    set2 = set(list2)\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    return intersection / union if union != 0 else 0\n",
    "\n",
    "def overlap_coefficient(list1, list2):\n",
    "    if list1 is None or list2 is None:\n",
    "        return None\n",
    "    set1 = set(list1)\n",
    "    set2 = set(list2)\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    return intersection / min(len(set1), len(set2)) if min(len(set1), len(set2)) != 0 else 0\n",
    "\n",
    "token_cols = [\n",
    "    'description_tokens', \n",
    "    'description_en_tokens',\n",
    "    'description_mix_tokens',\n",
    "    'name_tokens', \n",
    "    'name_en_tokens',\n",
    "    'name_mix_tokens',\n",
    "    'description_tokens_w_digits', \n",
    "    'name_tokens_w_digits'\n",
    "]\n",
    "\n",
    "collection_cols = [\n",
    "    'attr_keys', \n",
    "    'attr_vals', \n",
    "    'units_name',\n",
    "    'units_desc',\n",
    "    'brands_name',\n",
    "    'brands_desc',\n",
    "    # 'colors_name',\n",
    "    # 'colors_desc'\n",
    "]\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    for col in token_cols:\n",
    "        total = len(df)\n",
    "        pbar = tqdm(total=total, desc=f\"Processing {col} jaccard and overlap scores\")\n",
    "\n",
    "        def jaccard_score(row):\n",
    "            pbar.update(1)\n",
    "            val1 = row[f'{col}_1']\n",
    "            val2 = row[f'{col}_2']\n",
    "            if val1 is None or val2 is None:\n",
    "                return 0.0\n",
    "            try:\n",
    "                list1 = val1.split()\n",
    "                list2 = val2.split()\n",
    "                return jaccard_similarity(list1, list2)\n",
    "            except (AttributeError, TypeError):\n",
    "                print('bad!!!')\n",
    "                return 0.0\n",
    "\n",
    "        def overlap_score(row):\n",
    "            val1 = row[f'{col}_1']\n",
    "            val2 = row[f'{col}_2']\n",
    "            if val1 is None or val2 is None:\n",
    "                return 0.0\n",
    "            try:\n",
    "                list1 = val1.split()\n",
    "                list2 = val2.split()\n",
    "                return overlap_coefficient(list1, list2)\n",
    "            except (AttributeError, TypeError):\n",
    "                print('bad!!!')\n",
    "                return 0.0\n",
    "\n",
    "        df = df.with_columns(\n",
    "            pl.struct([f'{col}_1', f'{col}_2']).map_elements(jaccard_score, return_dtype=pl.Float64).alias(f'{col}_jaccard_score'),\n",
    "            pl.struct([f'{col}_1', f'{col}_2']).map_elements(overlap_score, return_dtype=pl.Float64).alias(f'{col}_overlap_score')\n",
    "        )\n",
    "        pbar.close()\n",
    "\n",
    "    for col in collection_cols:\n",
    "        total = len(df)\n",
    "        pbar = tqdm(total=total, desc=f\"Processing {col} jaccard and overlap scores\")\n",
    "\n",
    "        def jaccard_score_collection(row):\n",
    "            pbar.update(1)\n",
    "            val1 = row[f'{col}_1']\n",
    "            val2 = row[f'{col}_2']\n",
    "            if val1 is None or val2 is None:\n",
    "                return 0.0\n",
    "            try:\n",
    "                return jaccard_similarity(val1, val2)\n",
    "            except Exception:\n",
    "                print('bad!!!')\n",
    "                return 0.0\n",
    "\n",
    "        def overlap_score_collection(row):\n",
    "            val1 = row[f'{col}_1']\n",
    "            val2 = row[f'{col}_2']\n",
    "            if val1 is None or val2 is None:\n",
    "                return 0.0\n",
    "            try:\n",
    "                return overlap_coefficient(val1, val2)\n",
    "            except Exception:\n",
    "                print('bad!!!')\n",
    "                return 0.0\n",
    "\n",
    "        df = df.with_columns(\n",
    "            pl.struct([f'{col}_1', f'{col}_2']).map_elements(jaccard_score_collection, return_dtype=pl.Float64).alias(f'{col}_jaccard_score'),\n",
    "            pl.struct([f'{col}_1', f'{col}_2']).map_elements(overlap_score_collection, return_dtype=pl.Float64).alias(f'{col}_overlap_score')\n",
    "        )\n",
    "        pbar.close()\n",
    "        \n",
    "    if name == 'train_df':\n",
    "        train_df = df\n",
    "    else:\n",
    "        test_df = df\n",
    "        \n",
    "    dataframes = {'train_df': train_df, 'test_df': test_df}\n",
    "        \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6394f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# количество совпадющих ключей и значений атрибутов\n",
    "\n",
    "collection_cols_for_common_count = ['attr_keys', 'attr_vals']\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    for col in collection_cols_for_common_count:\n",
    "        total = len(df)\n",
    "        pbar = tqdm(total=total, desc=f\"Processing {col} common elements count\")\n",
    "\n",
    "        def common_elements_count(row):\n",
    "            pbar.update(1)\n",
    "            val1 = row[f'{col}_1']\n",
    "            val2 = row[f'{col}_2']\n",
    "            if val1 is None or val2 is None:\n",
    "                return 0\n",
    "            try:\n",
    "                set1 = set(val1)\n",
    "                set2 = set(val2)\n",
    "                return len(set1.intersection(set2))\n",
    "            except Exception:\n",
    "                print('bad!!!')\n",
    "                return 0\n",
    "\n",
    "        df = df.with_columns(\n",
    "            pl.struct([f'{col}_1', f'{col}_2']).map_elements(common_elements_count, return_dtype=pl.Int64).alias(f'{col}_common_count')\n",
    "        )\n",
    "        pbar.close()\n",
    "        \n",
    "    if name == 'train_df':\n",
    "        train_df = df\n",
    "    else:\n",
    "        test_df = df\n",
    "        \n",
    "    dataframes = {'train_df': train_df, 'test_df': test_df}\n",
    "        \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2818989d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bm25 на name+desc\n",
    "\n",
    "def tokenize(text):\n",
    "    if text is None:\n",
    "        return []\n",
    "    return re.findall(r'\\w+', str(text).lower())\n",
    "\n",
    "def calculate_bm25_score(df):\n",
    "    total = len(df)\n",
    "    pbar = tqdm(total=total*2, desc='Calculating BM25Okapi scores')\n",
    "    \n",
    "    def bm25_score_left(row):\n",
    "        pbar.update(1)\n",
    "        \n",
    "        name_1 = row['name_1']\n",
    "        desc_1 = row['description_1']\n",
    "        name_2 = row['name_2']\n",
    "        desc_2 = row['description_2']\n",
    "        \n",
    "        name_1 = \"\" if name_1 is None else str(name_1)\n",
    "        desc_1 = \"\" if desc_1 is None else str(desc_1)\n",
    "        name_2 = \"\" if name_2 is None else str(name_2)\n",
    "        desc_2 = \"\" if desc_2 is None else str(desc_2)\n",
    "        \n",
    "        combined_1 = f\"Name: {name_1}, Desc: {desc_1}\"\n",
    "        combined_2 = f\"Name: {name_2}, Desc: {desc_2}\"\n",
    "        \n",
    "        tokens_1 = tokenize(combined_1)\n",
    "        tokens_2 = tokenize(combined_2)\n",
    "        \n",
    "        if not tokens_1 or not tokens_2:\n",
    "            return 0.0\n",
    "        \n",
    "        bm25 = BM25Okapi([tokens_2])\n",
    "        scores = bm25.get_scores(tokens_1)\n",
    "        \n",
    "        return float(scores[0]) if len(scores) > 0 else 0.0\n",
    "    \n",
    "    def bm25_score_right(row):\n",
    "        pbar.update(1)\n",
    "        \n",
    "        name_1 = row['name_2']\n",
    "        desc_1 = row['description_2']\n",
    "        name_2 = row['name_1']\n",
    "        desc_2 = row['description_1']\n",
    "        \n",
    "        name_1 = \"\" if name_1 is None else str(name_1)\n",
    "        desc_1 = \"\" if desc_1 is None else str(desc_1)\n",
    "        name_2 = \"\" if name_2 is None else str(name_2)\n",
    "        desc_2 = \"\" if desc_2 is None else str(desc_2)\n",
    "        \n",
    "        combined_1 = f\"Name: {name_1}, Desc: {desc_1}\"\n",
    "        combined_2 = f\"Name: {name_2}, Desc: {desc_2}\"\n",
    "        \n",
    "        tokens_1 = tokenize(combined_1)\n",
    "        tokens_2 = tokenize(combined_2)\n",
    "        \n",
    "        if not tokens_1 or not tokens_2:\n",
    "            return 0.0\n",
    "        \n",
    "        bm25 = BM25Okapi([tokens_2])\n",
    "        scores = bm25.get_scores(tokens_1)\n",
    "        \n",
    "        return float(scores[0]) if len(scores) > 0 else 0.0\n",
    "    \n",
    "    df = df.with_columns([\n",
    "        pl.struct(['name_1', 'description_1', 'name_2', 'description_2'])\n",
    "        .map_elements(bm25_score_left, return_dtype=pl.Float64)\n",
    "        .alias('bm25_name_desc_score_left'),\n",
    "        pl.struct(['name_1', 'description_1', 'name_2', 'description_2'])\n",
    "        .map_elements(bm25_score_right, return_dtype=pl.Float64)\n",
    "        .alias('bm25_name_desc_score_right')\n",
    "    ])\n",
    "    pbar.close()\n",
    "    \n",
    "    return df\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    df = calculate_bm25_score(df)\n",
    "    \n",
    "    if name == 'train_df':\n",
    "        train_df = df\n",
    "    else:\n",
    "        test_df = df\n",
    "        \n",
    "    dataframes = {'train_df': train_df, 'test_df': test_df}\n",
    "        \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91014210",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# iou по n-gram\n",
    "\n",
    "def calculate_ngram_similarities_multiple_cols(df, cols):\n",
    "    total = len(df) * len(cols)\n",
    "    pbar = tqdm(total=total, desc='Calculating n-gram Similarities for multiple columns')\n",
    "    \n",
    "    def get_ngrams(text, n):\n",
    "        text = text.lower()\n",
    "        return [text[i:i+n] for i in range(len(text)-n+1)]\n",
    "    \n",
    "    def ngram_similarities(row, col):\n",
    "        pbar.update(1)\n",
    "        val_1 = \"\" if row[f'{col}_1'] is None else str(row[f'{col}_1'])\n",
    "        val_2 = \"\" if row[f'{col}_2'] is None else str(row[f'{col}_2'])\n",
    "        \n",
    "        sims = []\n",
    "        for n in [1, 2, 3, 4, 5, 6, 7]:\n",
    "            ngrams_1 = set(get_ngrams(val_1, n)) if len(val_1) >= n else set()\n",
    "            ngrams_2 = set(get_ngrams(val_2, n)) if len(val_2) >= n else set()\n",
    "            sim = 0.0\n",
    "            if ngrams_1 and ngrams_2:\n",
    "                intersection = len(ngrams_1 & ngrams_2)\n",
    "                union = len(ngrams_1 | ngrams_2)\n",
    "                sim = intersection / union if union > 0 else 0.0\n",
    "            sims.append(sim)\n",
    "        return sims\n",
    "    \n",
    "    for col in cols:\n",
    "        df = df.with_columns(\n",
    "            pl.struct([f'{col}_1', f'{col}_2'])\n",
    "            .map_elements(lambda row: ngram_similarities(row, col), return_dtype=pl.List(pl.Float64))\n",
    "            .alias(f'ngram_similarities_{col}')\n",
    "        )\n",
    "    \n",
    "    for col in cols:\n",
    "        df = df.with_columns([\n",
    "            pl.col(f'ngram_similarities_{col}').list.get(0).alias(f'{col}_char_1gram_iou'),\n",
    "            pl.col(f'ngram_similarities_{col}').list.get(1).alias(f'{col}_char_2gram_iou'),\n",
    "            pl.col(f'ngram_similarities_{col}').list.get(2).alias(f'{col}_char_3gram_iou'),\n",
    "            pl.col(f'ngram_similarities_{col}').list.get(3).alias(f'{col}_char_4gram_iou'),\n",
    "            pl.col(f'ngram_similarities_{col}').list.get(4).alias(f'{col}_char_5gram_iou'),\n",
    "            pl.col(f'ngram_similarities_{col}').list.get(5).alias(f'{col}_char_6gram_iou'),\n",
    "            pl.col(f'ngram_similarities_{col}').list.get(6).alias(f'{col}_char_7gram_iou'),\n",
    "        ]).drop(f'ngram_similarities_{col}')\n",
    "    \n",
    "    pbar.close()\n",
    "    return df\n",
    "\n",
    "columns_ngram = [\n",
    "    'name',\n",
    "    'name_en', \n",
    "    'name_mix',\n",
    "    'name_tokens_w_digits',\n",
    "    'description',\n",
    "    'description_en', \n",
    "    'description_mix', \n",
    "    'description_tokens_w_digits'\n",
    "]\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    df = calculate_ngram_similarities_multiple_cols(df, columns_ngram)\n",
    "    \n",
    "    if name == 'train_df':\n",
    "        train_df = df\n",
    "    else:\n",
    "        test_df = df\n",
    "        \n",
    "    dataframes = {'train_df': train_df, 'test_df': test_df}\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee607c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rouge по name+desc, очень долго\n",
    "\n",
    "def calculate_rouge_metrics(df):\n",
    "    total = len(df)\n",
    "    pbar = tqdm(total=total, desc='Calculating ROUGE Metrics')\n",
    "    \n",
    "    def rouge_scores(row):\n",
    "        pbar.update(1)\n",
    "        name_1 = \"\" if row['name_1'] is None else str(row['name_1'])\n",
    "        desc_1 = \"\" if row['description_1'] is None else str(row['description_1'])\n",
    "        name_2 = \"\" if row['name_2'] is None else str(row['name_2'])\n",
    "        desc_2 = \"\" if row['description_2'] is None else str(row['description_2'])\n",
    "        \n",
    "        combined_1 = f\"{name_1} {desc_1}\".strip()\n",
    "        combined_2 = f\"{name_2} {desc_2}\".strip()\n",
    "        \n",
    "        if len(combined_1) < 1 or len(combined_2) < 1:\n",
    "            return [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "        \n",
    "        try:\n",
    "            rouge = Rouge()\n",
    "            scores = rouge.get_scores(combined_1, combined_2)[0]\n",
    "            \n",
    "            rouge1_f = scores['rouge-1']['f']\n",
    "            rouge1_p = scores['rouge-1']['p']\n",
    "            rouge1_r = scores['rouge-1']['r']\n",
    "            \n",
    "            rouge2_f = scores['rouge-2']['f']\n",
    "            \n",
    "            rougeL_f = scores['rouge-l']['f']\n",
    "            rougeL_r = scores['rouge-l']['r']\n",
    "            \n",
    "            return [rouge1_f, rouge1_p, rouge1_r, rouge2_f, rougeL_f, rougeL_r]\n",
    "        except Exception:\n",
    "            return [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "    \n",
    "    df = df.with_columns(\n",
    "        pl.struct(['name_1', 'description_1', 'name_2', 'description_2'])\n",
    "        .map_elements(rouge_scores, return_dtype=pl.List(pl.Float64))\n",
    "        .alias('rouge_metrics')\n",
    "    )\n",
    "    \n",
    "    df = df.with_columns([\n",
    "        pl.col('rouge_metrics').list.get(0).alias('rouge1_f'),\n",
    "        pl.col('rouge_metrics').list.get(1).alias('rouge1_precision'),\n",
    "        pl.col('rouge_metrics').list.get(2).alias('rouge1_recall'),\n",
    "        pl.col('rouge_metrics').list.get(3).alias('rouge2_f'),\n",
    "        pl.col('rouge_metrics').list.get(4).alias('rougeL_f'),\n",
    "        pl.col('rouge_metrics').list.get(5).alias('rougeL_recall')\n",
    "    ]).drop('rouge_metrics')\n",
    "    \n",
    "    pbar.close()\n",
    "    return df\n",
    "\n",
    "# for name, df in dataframes.items():\n",
    "#     df = calculate_rouge_metrics(df)\n",
    "    \n",
    "#     if name == 'train_df':\n",
    "#         train_df = df\n",
    "#     else:\n",
    "#         test_df = df\n",
    "        \n",
    "#     dataframes = {'train_df': train_df, 'test_df': test_df}\n",
    "#     gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c379ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# удалим ненужные столбцы\n",
    "\n",
    "to_drop = [\n",
    "    'name_1', 'name_2', \n",
    "    'description_1', 'description_2', \n",
    "    'name_norm_1', 'name_norm_2',\n",
    "    'description_norm_1', 'description_norm_2', \n",
    "    'attr_vals_1', 'attr_vals_2',\n",
    "    'attr_keys_1', 'attr_keys_2',\n",
    "    'characteristic_attributes_mapping_1', 'characteristic_attributes_mapping_2',\n",
    "    'description_tokens_1', 'description_tokens_2',\n",
    "    'name_tokens_1', 'name_tokens_2', \n",
    "    'description_tokens_w_digits_1', 'description_tokens_w_digits_2',\n",
    "    'price_1', 'price_2',\n",
    "    'n_images_1', 'n_images_2',\n",
    "    'name_en_1', 'name_en_2',\n",
    "    'name_en_norm_1', 'name_en_norm_2',\n",
    "    'name_en_tokens_1', 'name_en_tokens_2',\n",
    "    'name_mix_1', 'name_mix_2',\n",
    "    'name_mix_norm_1', 'name_mix_norm_2',\n",
    "    'name_mix_tokens_1', 'name_mix_tokens_2',\n",
    "    'description_en_1', 'description_en_2',\n",
    "    'description_en_norm_1', 'description_en_norm_2',\n",
    "    'description_en_tokens_1', 'description_en_tokens_2',\n",
    "    'description_mix_1', 'description_mix_2',\n",
    "    'description_mix_norm_1', 'description_mix_norm_2',\n",
    "    'description_mix_tokens_1', 'description_mix_tokens_2',\n",
    "    'name_tokens_len_1', 'name_tokens_len_2',\n",
    "    'description_tokens_len_1', 'description_tokens_len_2',\n",
    "    'name_en_tokens_len_1', 'name_en_tokens_len_2',\n",
    "    'description_en_tokens_len_1', 'description_en_tokens_len_2',\n",
    "    'name_mix_tokens_len_1', 'name_mix_tokens_len_2',\n",
    "    'description_mix_tokens_len_1', 'description_mix_tokens_len_2',\n",
    "    'attr_keys_len_1', 'attr_keys_len_2',\n",
    "    'attr_vals_len_1', 'attr_vals_len_2',\n",
    "    'units_name_1', 'units_name_2',\n",
    "    'units_desc_1', 'units_desc_2',\n",
    "    'brands_name_1', 'brands_name_2',\n",
    "    'brands_desc_1', 'brands_desc_2',\n",
    "    'colors_name_1', 'colors_name_2',\n",
    "    'colors_desc_1', 'colors_desc_2',\n",
    "    'name_tokens_w_digits_1', 'name_tokens_w_digits_2'\n",
    "]\n",
    "\n",
    "train_df = train_df.drop(to_drop)\n",
    "test_df = test_df.drop(to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c79deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.write_parquet('../data/merged-with-features/train_df.parquet')\n",
    "test_df.write_parquet('../data/merged-with-features/test_df.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
