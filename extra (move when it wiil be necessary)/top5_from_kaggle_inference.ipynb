{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08675177-bda8-41ce-ab79-74030dbb258b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import duckdb\n",
    "import albumentations\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "import torch\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "\n",
    "class ShopeeModel(nn.Module):\n",
    "    def __init__(self, model_name, pretrained, fc_dim=512):\n",
    "        super(ShopeeModel, self).__init__()\n",
    "        self.model_name = model_name\n",
    "        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n",
    "        if model_name != \"tf_efficientnet_b4\":\n",
    "            in_features = self.backbone.head.in_features\n",
    "            self.backbone.head = nn.Identity()\n",
    "        else:\n",
    "            in_features = self.backbone.classifier.in_features\n",
    "            self.backbone.classifier = nn.Identity()\n",
    "        self.backbone.global_pool = nn.Identity()\n",
    "        self.pooling = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.classifier = nn.Linear(in_features, fc_dim)\n",
    "        self.bn = nn.BatchNorm1d(fc_dim)\n",
    "        self._init_params()\n",
    "        self.vector_size = fc_dim\n",
    "\n",
    "    def _init_params(self):\n",
    "        nn.init.xavier_normal_(self.classifier.weight)\n",
    "        nn.init.constant_(self.classifier.bias, 0)\n",
    "        nn.init.constant_(self.bn.weight, 1)\n",
    "        nn.init.constant_(self.bn.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.backbone(x)\n",
    "        if self.model_name == \"tf_efficientnet_b4\":\n",
    "            x = self.pooling(x)\n",
    "        x = x.view(batch_size, -1)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        x = self.classifier(x)\n",
    "        x = self.bn(x)\n",
    "\n",
    "        x = F.normalize(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "def get_images_name(set_type):\n",
    "    with open(f'./valid_{set_type}_images.txt', 'r') as f:\n",
    "        images = list(\n",
    "            map(\n",
    "                lambda x: x.strip(),\n",
    "                f.readlines()\n",
    "            )\n",
    "        )\n",
    "    return images\n",
    "\n",
    "\n",
    "def load_and_preprocess(path):\n",
    "    img = cv2.imread(path)\n",
    "\n",
    "    outs = [torch.FloatTensor(cv2.resize(img, size)).permute(2, 0, 1) / 255.0 for size in sizes]\n",
    "    return outs\n",
    "\n",
    "\n",
    "def split_set_into_n_chunks(txt_with_images_paths, n):\n",
    "    with open(txt_with_images_paths, 'r') as f:\n",
    "        paths = list(\n",
    "            map(\n",
    "                lambda x: x.strip(),\n",
    "                f.readlines()\n",
    "            )\n",
    "        )\n",
    "    paths.sort()\n",
    "    chunk_size = len(paths) // n\n",
    "    remainder = len(paths) % n\n",
    "\n",
    "    chunks = []\n",
    "    start = 0\n",
    "\n",
    "    for i in range(n):\n",
    "        end = start + chunk_size + (1 if i < remainder else 0)\n",
    "        chunks.append(paths[start:end])\n",
    "        start = end\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def batch_images_generator(path_to_folder, bs, resume_from_batch=None, list_of_images=None):\n",
    "    if list_of_images:\n",
    "        images = list_of_images\n",
    "    else:\n",
    "        images = sorted(glob.glob('*.jpg', root_dir=path_to_folder))\n",
    "        images = [image for image in images if os.path.getsize(path_to_folder + image) > 0]\n",
    "\n",
    "    print(len(images))\n",
    "    start = 0 * bs if not resume_from_batch else resume_from_batch * bs\n",
    "    print(f'start from {start}')\n",
    "\n",
    "    for i in tqdm.tqdm(range(start, len(images), bs)):\n",
    "        batch_images = images[i:i + bs]\n",
    "\n",
    "        batches = [\n",
    "            [] for _ in range(len(models))\n",
    "        ]\n",
    "\n",
    "        valid_images = []\n",
    "        for image in batch_images:\n",
    "            try:\n",
    "                preprocessed = load_and_preprocess(path_to_folder + image)\n",
    "\n",
    "                for k, preprocessed_image in enumerate(preprocessed):\n",
    "                    batches[k].append(preprocessed_image.unsqueeze(0))\n",
    "\n",
    "                valid_images.append(image)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(image)\n",
    "\n",
    "        yield i, valid_images, batches\n",
    "\n",
    "\n",
    "def save_parquet(filenames, embedded, path, i):\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"filename\": filenames,\n",
    "        \"embedding\": embedded\n",
    "    })\n",
    "\n",
    "    df.to_parquet(\n",
    "        os.path.join(path, f\"batch_{i // bs}.parquet\"),\n",
    "        index=False\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c346620d-1b3d-4f13-b6a7-d6ae4f70238a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class CFG:\n",
    "    seed = 54\n",
    "    classes = 11014\n",
    "    scale = 30\n",
    "    margin = 0.5\n",
    "    model_name = 'tf_efficientnet_b4'\n",
    "    fc_dim = 512\n",
    "    img_size = 512\n",
    "    batch_size = 20\n",
    "    num_workers = 4\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "img_backbones = [\n",
    "    #\"swin_base_patch4_window12_384\", # 384\n",
    "    'tf_efficientnet_b4', # 512\n",
    "    \"vit_base_r50_s16_384\" # 384\n",
    "]\n",
    "\n",
    "img_model_paths = [\n",
    "    #'./weights/top5/img_model_i15.pth',\n",
    "    './weights/top5/img_model_i04.pth',\n",
    "    './weights/top5/img_model_i11.pth'\n",
    "]\n",
    "\n",
    "models = [None for _ in range(len(img_backbones))]\n",
    "\n",
    "sizes = [\n",
    "    #(384, 384),\n",
    "    (512, 512),\n",
    "    (384, 384)\n",
    "]\n",
    "\n",
    "for i in range(len(img_backbones)):\n",
    "    try:\n",
    "        models[i] = ShopeeModel(img_backbones[i],  pretrained=False).to(CFG.device)\n",
    "        models[i].load_state_dict(torch.load(img_model_paths[i]))\n",
    "        models[i].eval()\n",
    "        print(f'{i} completed')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad782ab-449a-437d-8a1d-2fc83d920b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 128\n",
    "N_CHUNKS = 4\n",
    "CURRENT_CHUNK_TO_PROCESS = 0\n",
    "SET_TYPE = 'train'\n",
    "\n",
    "images = get_images_name(SET_TYPE)[:1000]\n",
    "path = rf'C:\\avito\\images/{SET_TYPE}/images/'\n",
    "output_dir = fr'C:\\avito\\images\\{SET_TYPE}\\parquets/'\n",
    "path_to_zip_paths = f'./{SET_TYPE}_images_zip_paths.txt'\n",
    "#chunks = split_set_into_n_chunks(path_to_zip_paths, N_CHUNKS)\n",
    "#images = chunks[CURRENT_CHUNK_TO_PROCESS]\n",
    "\n",
    "for model_name in img_backbones:\n",
    "    os.makedirs(output_dir + model_name, exist_ok=True)\n",
    "\n",
    "os.makedirs(output_dir + 'concat', exist_ok=True)\n",
    "\n",
    "for i, filenames, out in batch_images_generator(path_to_folder=path,\n",
    "                                                bs=bs,\n",
    "                                                list_of_images=images):\n",
    "\n",
    "    result = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for k, batch in enumerate(out):\n",
    "\n",
    "            batch = torch.cat(batch)\n",
    "            batch = batch.to(device)\n",
    "\n",
    "            embedded = models[k](batch).cpu().numpy()\n",
    "\n",
    "            result.append(embedded)\n",
    "\n",
    "            del embedded\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    for k, res in enumerate(result):\n",
    "        save_parquet(filenames, res.tolist(), output_dir + img_backbones[k], i)\n",
    "\n",
    "    if len(result) > 1:\n",
    "        concat = np.concatenate(result, axis=1)\n",
    "        save_parquet(filenames, concat.tolist(), output_dir + 'concat', i)\n",
    "\n",
    "for model_name in img_backbones:\n",
    "    duckdb.sql(rf\"\"\"\n",
    "        COPY (\n",
    "            SELECT * FROM '{output_dir + model_name}/*.parquet'\n",
    "        )\n",
    "        TO '{output_dir}/final_{model_name}_chunk{CURRENT_CHUNK_TO_PROCESS}.parquet' (FORMAT PARQUET)\n",
    "    \"\"\")\n",
    "duckdb.sql(rf\"\"\"\n",
    "    COPY (\n",
    "        SELECT * FROM '{output_dir + 'concat'}/*.parquet'\n",
    "    )\n",
    "    TO '{output_dir}/final_concat_chunk{CURRENT_CHUNK_TO_PROCESS}.parquet' (FORMAT PARQUET)\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
